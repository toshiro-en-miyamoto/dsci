<!DOCTYPE html>
<html lang="ja">
<head>
    <title>Statistics by Brotanica</title>
    <link rel="stylesheet" href="../../style/dark.css" type="text/css">
    <link rel="stylesheet" href="../../style/indent.css" type="text/css">
</head>
<body>

<p>
    <strong>Reference</strong>:
    <a href="https://www.britannica.com/science/statistics">
        Statistics by Brotanica
    </a>
</p>

<!-- ******************************************************************** -->
<h1>Introduction</h1>
<section id="introduction" class="description">

<p>
<strong>Statistics</strong>, the science of collecting, analyzing, presenting, and interpreting data.
</p>
<p>
Data are the facts and figures that are collected, analyzed, and summarized for presentation and interpretation. Data may be classified as either quantitative or qualitative:
</p>
<ul>
    <li>
        quantitative data measure either how much or how many of something, and
    </li>
    <li>
        qualitative data provide labels, or names, for categories of like items.
    </li>
</ul>
<p>
For example, suppose that a particular study is interested in characteristics such as age, gender, marital status, and annual income for a sample of 100 individuals. These characteristics would be called the <strong>variables</strong> of the study, and data values for each of the variables would be associated with each individual. Thus, the data values of 28, male, single, and $30,000 would be recorded for a 28-year-old single male with an annual income of $30,000. With 100 individuals and 4 variables, the data set would have
    <math>
        <mrow>
            <mn>100</mn>
            <mo>&times;</mo>
            <mn>4</mn>
            <mo>&equals;</mo>
            <mn>400</mn>
        </mrow>
    </math>
items. In this example,
</p>
<ul>
    <li>
        age and annual income are <strong>quantitative variables</strong>; the corresponding data values indicate how many years and how much money for each individual.
    </li>
    <li>
        Gender and marital status are <strong>qualitative variables</strong>. The labels male and female provide the qualitative data for gender, and the labels single, married, divorced, and widowed indicate marital status.
    </li>
</ul>
<p>
Sample survey methods are used to collect data from observational studies, and experimental design methods are used to collect data from experimental studies. The area of descriptive statistics is concerned primarily with methods of presenting and interpreting data using graphs, tables, and numerical summaries. Whenever statisticians use data from a <strong>sample</strong>&mdash;i.e., a subset of the <strong>population</strong> &mdash; to make statements about a population, they are performing statistical inference. Estimation and hypothesis testing are procedures used to make statistical inferences. Fields such as health care, biology, chemistry, physics, education, engineering, business, and economics make extensive use of statistical inference.
</p>
<p>
Methods of <strong>probability</strong> were developed initially for the analysis of gambling games. Probability plays a key role in statistical inference; it is used to provide measures of the quality and precision of the inferences. Many of the methods of statistical inference are described in this article. Some of these methods are used primarily for single-variable studies, while others, such as regression and correlation analysis, are used to make inferences about relationships among two or more variables.
</p>

</section>

<!-- ******************************************************************** -->
<h1>Descriptive statistics</h1>
<section id="descriptive_statistics" class="description">

<p>
<strong>Descriptive statistics</strong> are tabular, graphical, and numerical <em>summaries of data</em>. The purpose of descriptive statistics is to facilitate the <em>presentation</em> and <em>interpretation</em> of data. Most of the statistical presentations appearing in newspapers and magazines are descriptive in nature.
</p>
<ul>
    <li>
        Univariate methods of descriptive statistics use data to enhance the understanding of a <em>single variable</em>;
    </li>
    <li>
        multivariate methods focus on using statistics to understand the relationships among <em>two or more variables</em>.
    </li>
</ul>
<p>
To illustrate methods of descriptive statistics, the previous example in which data were collected on the age, gender, marital status, and annual income of 100 individuals will be examined.
</p>

</section>

<!-- ******************************************************************** -->
<h2>Tabular methods</h2>
<section id="tabular_methods" class="description">

<p>
The most commonly used tabular summary of data for a single variable is a <strong>frequency distribution</strong>. A frequency distribution shows the number of data values in each of several nonoverlapping classes. Another tabular summary, called a <strong>relative frequency distribution</strong>, shows the fraction, or percentage, of data values in each class. The most common tabular summary of data for two variables is a cross tabulation, a two-variable analogue of a frequency distribution.
</p>
<p>
For a <em>qualitative</em> variable, a frequency distribution shows the number of data values in each qualitative <strong>category</strong>. For instance, the variable gender has two categories: male and female. Thus, a frequency distribution for gender would have two nonoverlapping classes to show the number of males and females. A relative frequency distribution for this variable would show the fraction of individuals that are male and the fraction of individuals that are female.
</p>
<p>
Constructing a frequency distribution for a <em>quantitative</em> variable requires more care in defining the <strong>classes</strong> and the division points between adjacent classes. For instance, if the age data of the example above ranged from 22 to 78 years, the following six nonoverlapping classes could be used: 20–29, 30–39, 40–49, 50–59, 60–69, and 70–79. A frequency distribution would show the number of data values in each of these classes, and a relative frequency distribution would show the fraction of data values in each.
</p>
<p>
A <strong>cross tabulation</strong> is a two-way table with the rows of the table representing the <em>classes of one variable</em> and the columns of the table representing the <em>classes of another variable</em>. To construct a cross tabulation using the variables gender and age, gender could be shown with two rows, male and female, and age could be shown with six columns corresponding to the age classes 20–29, 30–39, 40–49, 50–59, 60–69, and 70–79. The entry in each cell of the table would specify the number of data values with the gender given by the row heading and the age given by the column heading. Such a cross tabulation could be helpful in understanding the <em>relationship</em> between gender and age.
</p>

</section>

<!-- ******************************************************************** -->
<h2>Graphical methods</h2>
<section id="graphical_methods" class="description">

<p>
A <strong>bar graph</strong> is a graphical device for depicting qualitative data that have been summarized in a frequency distribution. Labels for <em>the categories of the qualitative variable</em> are shown on the horizontal axis of the graph. A bar above each label is constructed such that the height of each bar is <em>proportional</em> to the number of data values in the category.
</p>
<p>
A <strong>pie chart</strong> is another graphical device for summarizing <em>qualitative data</em>. The size of each slice of the pie is <em>proportional</em> to the number of data values in the corresponding <em>class</em>.
</p>
<p>
A <strong>histogram</strong> is the most common graphical presentation of quantitative data that have been summarized in a frequency distribution. The values of the quantitative variable are shown on the horizontal axis. A rectangle is drawn above each class such that the base of the rectangle is equal to the width of the <em>class interval</em> and its height is <em>proportional</em> to the number of data values in the class.
</p>

</section>

<!-- ******************************************************************** -->
<h2>Numerical measures</h2>
<section id="numerical_measures" class="description">

<p>
A variety of numerical measures are used to summarize data.
</p>
<ul>
    <li>
        The proportion, or percentage, of data values in each category is the primary numerical measure for <em>qualitative</em> data.
    </li>
    <li>
        The mean, median, mode, percentiles, range, variance, and standard deviation are the most commonly used numerical measures for <em>quantitative</em> data.
    </li>
</ul>
<p>
The <strong>mean</strong>, often called the average, is computed by adding all the data values for a variable and dividing the sum by the number of data values. The mean is a measure of the central location for the data.
</p>
<p>
The <strong>median</strong> is another measure of central location that, unlike the mean, is not affected by extremely large or extremely small data values. When determining the median, the data values are first ranked in order from the smallest value to the largest value. If there is an odd number of data values, the median is the middle value; if there is an even number of data values, the median is the average of the two middle values.
</p>
<p>
The third measure of central tendency is the <strong>mode</strong>, the data value that occurs with greatest frequency.
</p>
<p>
<strong>Percentiles</strong> provide an indication of how the data values are spread over the interval from the smallest value to the largest value. Approximately <var>p</var> percent of the data values fall below the <var>p</var>th percentile, and roughly
    <math>
        <mrow>
            <mo fence="true">&lpar;</mo>
            <mn>100</mn>
            <mo>&minus;</mo>
            <mi>p</mi>
            <mo fence="true">&rpar;</mo>
        </mrow>
    </math>
percent of the data values are above the <var>p</var>th percentile. Percentiles are reported, for example, on most standardized tests.
</p>
<p>
<strong>Quartiles</strong> divide the data values into four parts;
the <em>first quartile</em> is the 25th percentile,
the <em>second quartile</em> is the 50th percentile (also the median), and
the <em>third quartile</em> is the 75th percentile.
</p>
<p>
The <strong>range</strong>, the difference between the largest value and the smallest value, is the simplest measure of variability in the data. The range is determined by only the two extreme data values.
</p>
<p>
The <strong>variance</strong> <var>s<sup>2</sup></var> and the <strong>standard deviation</strong> (<var>s</var>), on the other hand, are measures of <strong>variability</strong> that are based on all the data and are more commonly used. Equation 1 shows the formula for computing the variance of a sample consisting of <var>n</var> items. In applying equation 1, the <strong>deviation</strong> (difference) of each data value from the <strong>sample mean</strong> is computed and squared. The squared deviations are then summed and divided by (<var>n</var> &minus; 1) to provide the <strong>sample variance</strong>.
</p>
<figure>
    <math displaystyle="true">
        <mrow>
            <msup>
                <mi>s</mi>
                <mn>2</mn>
            </msup>
        </mrow>
        <mo>&equals;</mo>
        <mrow>
            <mfrac>
                <mrow>
                    <msubsup>
                        <mi>&sum;</mi>
                        <mrow>
                            <mi>i</mi>
                            <mo>&equals;</mo>
                            <mn>1</mn>
                        </mrow>
                        <mi>n</mi>
                    </msubsup>
                    <mo>&ApplyFunction;</mo>
                    <msup>
                        <mrow>
                            <mo fence="true">&lpar;</mo>
                            <msub>
                                <mi>x</mi>
                                <mi>i</mi>
                            </msub>
                            <mo>&minus;</mo>
                            <mover>
                                <mi>x</mi>
                                <mo>&OverBar;</mo>
                            </mover>
                            <mo fence="true">&rpar;</mo>
                        </mrow>
                        <mn>2</mn>
                    </msup>
                </mrow>
                <mrow>
                    <mi>n</mi>
                    <mo>&minus;</mo>
                    <mn>1</mn>
                </mrow>
            </mfrac>
        </mrow>
        <mrow>
            <mspace width="3em"/>
            <mtext>(1)</mtext>
        </mrow>
    </math>
</figure>
<p>
The standard deviation is the square root of the variance. Because the unit of measure for the standard deviation is the same as the unit of measure for the data, many individuals prefer to use the standard deviation as the descriptive measure of variability.
</p>

<h3>Outliers</h3>
<p>
Sometimes data for a variable will include one or more values that appear unusually large or small and out of place when compared with the other data values. These values are known as <strong>outliers</strong> and often have been erroneously included in the data set.
</p>
<p>
Experienced statisticians take steps to identify outliers and then review each one carefully for accuracy and the appropriateness of its inclusion in the data set. If an error has been made, corrective action, such as <em>rejecting</em> the data value in question, can be taken.
</p>
<p>
The mean and standard deviation are used to identify outliers. A <strong><var>z</var>-score</strong> can be computed for each data value. With <var>x</var> representing the data value,
    <math>
        <mover>
            <mi>X</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>
the <em>sample mean</em>, and <var>s</var> the <em>sample standard deviation</em>, the <var>z</var>-score is given by
</p>
<figure>
    <math displaystyle="true">
        <mrow>
            <mi>z</mi>
            <mo>&equals;</mo>
            <mfrac>
                <mrow>
                    <mo fence="true">&lpar;</mo>
                    <mi>x</mi>
                    <mo>&minus;</mo>
                    <mover>
                        <mi>x</mi>
                        <mo>&OverBar;</mo>
                    </mover>
                    <mo fence="true">&rpar;</mo>
                </mrow>
                <mi>s</mi>
            </mfrac>
        </mrow>
    </math>
</figure>
<p>
The <var>z</var>-score represents the relative position of the data value by indicating the number of standard deviations it is from the mean. A rule of thumb is that any value with a <em><var>z</var>-score less than −3 or greater than +3</em> should be considered an outlier.
</p>

<h3>Exploratory data analysis</h3>
<p>
Exploratory data analysis provides a variety of tools for quickly summarizing and gaining insight about a set of data. Two such methods are the five-number summary and the box plot.
</p>
<p>
A <strong>five-number summary</strong> simply consists of
<ul>
    <li>the smallest data value,</li>
    <li>the first quartile,</li>
    <li>the median,</li>
    <li>the third quartile, and</li>
    <li>the largest data value.</li>
</ul>
</p>
<p>
A <strong>box plot</strong> is a graphical device based on a five-number summary. A rectangle (i.e., the box) is drawn with the ends of the rectangle located at the first and third quartiles. The rectangle represents the middle 50 percent of the data. A vertical line is drawn in the rectangle to locate the median. Finally lines, called <strong>whiskers</strong>, extend from one end of the rectangle to the smallest data value and from the other end of the rectangle to the largest data value. If outliers are present, the whiskers generally extend only to the smallest and largest data values that are not outliers. Dots, or asterisks, are then placed outside the whiskers to denote the presence of outliers.
</p>

</section>

<!-- ******************************************************************** -->
<h1>Probability</h1>
<section id="probability" class="description">

<p>
Probability is a subject that deals with uncertainty. In everyday terminology, probability can be thought of as a numerical measure of the likelihood that a particular <strong>event</strong> will occur. Probability values are assigned on a scale from 0 to 1, with values near 0 indicating that an event is unlikely to occur and those near 1 indicating that an event is likely to take place. A probability of 0.50 means that an event is equally likely to occur as not to occur.
</p>

</section>

<!-- ******************************************************************** -->
<h2>Events and their probabilities</h2>
<section id="events_probabilities" class="description">

<p>
Oftentimes probabilities need to be computed for related events. For instance, advertisements are developed for the purpose of increasing sales of a product.
</p>
<ul>
    <li>
        If seeing the advertisement increases the probability of a person buying the product, the events “seeing the advertisement” and “buying the product” are said to be <strong>dependent</strong>.
    </li>
    <li>
        If two events are <strong>independent</strong>, the occurrence of one event does not affect the probability of the other event taking place. When two or more events are independent, the probability of their joint occurrence is the <em>product of their individual probabilities</em>.
    </li>
    <li>
        Two events are said to be <strong>mutually exclusive</strong> if the occurrence of one event means that the other event cannot occur; in this case, when one event takes place, the probability of the other event occurring is zero.
    </li>
</ul>

</section>

<!-- ******************************************************************** -->
<h2>Random variables and probability distributions</h2>
<section id="probability_distributions" class="description">

<p>
A <strong>random variable</strong> is a numerical description of the <em>outcome of a statistical experiment</em>. A random variable that may assume only a finite number or an infinite sequence of values is said to be discrete; one that may assume any value in some interval on the real number line is said to be continuous. For instance, a random variable representing the number of automobiles sold at a particular dealership on one day would be discrete, while a random variable representing the weight of a person in kilograms (or pounds) would be continuous.
</p>
<p>
The <strong>probability distribution</strong> for a random variable describes how the probabilities are distributed over the values of the random variable.
</p>
<p>
For a <em>discrete</em> random variable, <var>x</var>, the probability distribution is defined by a <strong>probability mass function</strong>, denoted by
    <math>
        <mi>f</mi>
        <mo>&ApplyFunction;</mo>
        <mo fence="true">&lpar;</mo>
        <mi>x</mi>
        <mo fence="true">&rpar;</mo>
    </math>.
This function provides the <em>probability for each value</em> of the random variable. In the development of the probability function for a discrete random variable, two conditions must be satisfied:
</p>
<ol>
    <li>
        <math>
            <mi>f</mi>
            <mo>&ApplyFunction;</mo>
            <mo fence="true">&lpar;</mo>
            <mi>x</mi>
            <mo fence="true">&rpar;</mo>
        </math>
        must be nonnegative for each value of the random variable, and
    </li>
    <li>
        the sum of the probabilities for each value of the random variable must equal one.
    </li>
</ol>
<p>
A continuous random variable may assume any value in an interval on the real number line or in a collection of intervals. Since there is an infinite number of values in any interval, it is not meaningful to talk about the probability that the random variable will take on a specific value; instead, the probability that a <em>continuous</em> random variable will lie within a <em>given interval</em> is considered.
</p>
<p>
In the continuous case, the counterpart of the probability mass function is the <strong>probability density function</strong>, also denoted by
    <math>
        <mi>f</mi>
        <mo>&ApplyFunction;</mo>
        <mo fence="true">&lpar;</mo>
        <mi>x</mi>
        <mo fence="true">&rpar;</mo>
    </math>.
For a continuous random variable, the probability density function provides the height or value of the function at any particular value of <var>x</var>; it does not directly give the probability of the random variable taking on a specific value. However, <em>the area</em> under the graph of
    <math>
        <mi>f</mi>
        <mo>&ApplyFunction;</mo>
        <mo fence="true">&lpar;</mo>
        <mi>x</mi>
        <mo fence="true">&rpar;</mo>
    </math>
corresponding to some interval, obtained by computing the integral of
    <math>
        <mi>f</mi>
        <mo>&ApplyFunction;</mo>
        <mo fence="true">&lpar;</mo>
        <mi>x</mi>
        <mo fence="true">&rpar;</mo>
    </math>
over that interval, provides the probability that the variable will take on a value within that interval. A probability density function must satisfy two requirements:
</p>
<ol>
    <li>
        <math>
            <mi>f</mi>
            <mo>&ApplyFunction;</mo>
            <mo fence="true">&lpar;</mo>
            <mi>x</mi>
            <mo fence="true">&rpar;</mo>
        </math>
        must be nonnegative for each value of the random variable, and
    </li>
    <li>
        the integral over all values of the random variable must equal one.
    </li>
</ol>

<aside class="wrapup">
<p>
According to the article
    <a href="https://en.wikipedia.org/wiki/Probability_density_function">
        Probability density function
    </a>,
the probability density function (PDF) is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one value. This probability is given by the integral of this variable's PDF over that range&mdash;that is, it is given by the <em>area</em> under the density function but above the horizontal axis and between the lowest and greatest values of the range.
</p>
<figure>
    <figcaption>
        Boxplot and probability density function of a normal distribution
        <math>
            <mi>N</mi>
            <mo>&ApplyFunction;</mo>
            <mo fence="true">&lpar;</mo>
            <mn>0</mn>
            <mo separator="true">&comma;</mo>
            <msup>
                <mi>&sigma;</mi>
                <mn>2</mn>
            </msup>
            <mo fence="true">&rpar;</mo>
        </math>.
        By Nishiguchi at en.wikipedia, CC BY-SA 2.5, 2006-09-22
    </figcaption>
    <img src="Boxplot_vs_PDF.png" alt="Boxplot and probability density function of a normal distribution">
</figure>
</aside>

<p>
The <strong>expected value</strong>, or mean, of a random variable &mdash; denoted by
    <math>
        <mi>E</mi>
        <mo>&ApplyFunction;</mo>
        <mo fence="true">&lpar;</mo>
        <mi>x</mi>
        <mo fence="true">&rpar;</mo>
    </math>
or <var>&mu;</var> &mdash; is a weighted average of the values the random variable may assume. In the discrete case the weights are given by the probability mass function, and in the continuous case the weights are given by the probability density function. The formulas for computing the expected values of discrete and continuous random variables are given by equations 2 and 3, respectively.
</p>
<figure>
    <math>
        <mrow>
            <mi>E</mi>
            <mo>&ApplyFunction;</mo>
            <mo fence="true">&lpar;</mo>
            <mi>x</mi>
            <mo fence="true">&rpar;</mo>
        </mrow>
        <mo>&equals;</mo>
        <mi>&mu;</mi>
        <mo>&equals;</mo>
        <mrow>
            <mo>&sum;</mo>
            <mo>&ApplyFunction;</mo>
            <mrow>
                <mi>x</mi>
                <mo>&InvisibleTimes;</mo>
                <mrow>
                    <mi>f</mi>
                    <mo>&ApplyFunction;</mo>
                    <mo fence="true">&lpar;</mo>
                    <mi>x</mi>
                    <mo fence="true">&rpar;</mo>
                </mrow>
            </mrow>
        </mrow>
        <mrow>
            <mspace width="3em"/>
            <mtext>(2)</mtext>
        </mrow>
    </math>
</figure>
<figure>
    <math>
        <mrow>
            <mi>E</mi>
            <mo>&ApplyFunction;</mo>
            <mo fence="true">&lpar;</mo>
            <mi>x</mi>
            <mo fence="true">&rpar;</mo>
        </mrow>
        <mo>&equals;</mo>
        <mi>&mu;</mi>
        <mo>&equals;</mo>
        <mrow>
            <mo>&Integral;</mo>
            <mo>&ApplyFunction;</mo>
            <mrow>
                <mi>x</mi>
                <mo>&InvisibleTimes;</mo>
                <mrow>
                    <mi>f</mi>
                    <mo>&ApplyFunction;</mo>
                    <mo fence="true">&lpar;</mo>
                    <mi>x</mi>
                    <mo fence="true">&rpar;</mo>
                </mrow>
                <mo>&InvisibleTimes;</mo>
                <mrow>
                    <mo>&PartialD;</mo>
                    <mi>x</mi>
                </mrow>
            </mrow>
        </mrow>
        <mrow>
            <mspace width="3em"/>
            <mtext>(3)</mtext>
        </mrow>
    </math>
</figure>
<p>
The <strong>variance</strong> of a random variable, denoted by
    <math>
        <mi>Var</mi>
        <mo>&ApplyFunction;</mo>
        <mo fence="true">&lpar;</mo>
        <mi>x</mi>
        <mo fence="true">&rpar;</mo>
    </math>
or <var>&sigma;<sup>2</sup></var>, is a weighted average of the squared deviations from the mean. In the discrete case the weights are given by the probability mass function, and in the continuous case the weights are given by the probability density function. The formulas for computing the variances of discrete and continuous random variables are given by equations 4 and 5, respectively.
</p>
<figure>
    <math>
        <mrow>
            <mi>Var</mi>
            <mo>&ApplyFunction;</mo>
            <mo fence="true">&lpar;</mo>
            <mi>x</mi>
            <mo fence="true">&rpar;</mo>
        </mrow>
        <mo>&equals;</mo>
        <msup>
            <mi>&sigma;</mi>
            <mn>2</mn>
        </msup>
        <mo>&equals;</mo>
        <mrow>
            <mo>&sum;</mo>
            <mo>&ApplyFunction;</mo>
            <mrow>
                <msup>
                    <mrow>
                        <mo fence="true">&lpar;</mo>
                        <mi>x</mi>
                        <mo>&minus;</mo>
                        <mi>&mu;</mi>
                        <mo fence="true">&rpar;</mo>
                    </mrow>
                    <mn>2</mn>
                </msup>
                <mo>&InvisibleTimes;</mo>
                <mrow>
                    <mi>f</mi>
                    <mo>&ApplyFunction;</mo>
                    <mo fence="true">&lpar;</mo>
                    <mi>x</mi>
                    <mo fence="true">&rpar;</mo>
                </mrow>
            </mrow>
        </mrow>
        <mrow>
            <mspace width="3em"/>
            <mtext>(4)</mtext>
        </mrow>
    </math>
</figure>
<figure>
    <math>
        <mrow>
            <mi>Var</mi>
            <mo>&ApplyFunction;</mo>
            <mo fence="true">&lpar;</mo>
            <mi>x</mi>
            <mo fence="true">&rpar;</mo>
        </mrow>
        <mo>&equals;</mo>
        <msup>
            <mi>&sigma;</mi>
            <mn>2</mn>
        </msup>
        <mo>&equals;</mo>
        <mrow>
            <mo>&Integral;</mo>
            <mo>&ApplyFunction;</mo>
            <mrow>
                <msup>
                    <mrow>
                        <mo fence="true">&lpar;</mo>
                        <mi>x</mi>
                        <mo>&minus;</mo>
                        <mi>&mu;</mi>
                        <mo fence="true">&rpar;</mo>
                    </mrow>
                    <mn>2</mn>
                </msup>
                <mo>&InvisibleTimes;</mo>
                <mrow>
                    <mi>f</mi>
                    <mo>&ApplyFunction;</mo>
                    <mo fence="true">&lpar;</mo>
                    <mi>x</mi>
                    <mo fence="true">&rpar;</mo>
                </mrow>
                <mo>&InvisibleTimes;</mo>
                <mrow>
                    <mo>&PartialD;</mo>
                    <mi>x</mi>
                </mrow>
            </mrow>
        </mrow>
        <mrow>
            <mspace width="3em"/>
            <mtext>(5)</mtext>
        </mrow>
    </math>
</figure>
<p>
The <strong>standard deviation</strong>, denoted <var>&sigma;</var>, is the positive square root of the variance. Since the standard deviation is measured in the same units as the random variable and the variance is measured in squared units, the standard deviation is often the preferred measure.
</p>

</section>

<!-- ******************************************************************** -->
<h2>Special probability distributions</h2>
<section id="special_probability_distributions" class="description">

<p>
Two of the most widely used discrete probability distributions are the binomial and Poisson.
</p>

<h3>The binomial distribution</h3>
<p>
The <strong>binomial</strong> probability mass function (equation 6) provides the probability that <var>x</var> successes will occur in <var>n</var> trials of a binomial experiment.
</p>
<figure>
    <math displaystyle="true">
        <mrow>
            <mi>f</mi>
            <mo>&ApplyFunction;</mo>
            <mo fence="true">&lpar;</mo>
            <mi>x</mi>
            <mo fence="true">&rpar;</mo>
        </mrow>
        <mo>&equals;</mo>
        <mrow>
            <mo fence="true">&lpar;</mo>
            <mfrac linethickness="0">
                <mi>n</mi>
                <mi>x</mi>
            </mfrac>
            <mo fence="true">&rpar;</mo>
            <mo>&InvisibleTimes;</mo>
            <msup>
                <mi>p</mi>
                <mi>x</mi>
            </msup>
            <mo>&InvisibleTimes;</mo>
            <msup>
                <mrow>
                    <mo fence="true">&lpar;</mo>
                    <mn>1</mn>
                    <mo>&minus;</mo>
                    <mi>p</mi>
                    <mo fence="true">&rpar;</mo>
                </mrow>
                <mrow>
                    <mi>n</mi>
                    <mo>&minus;</mo>
                    <mi>x</mi>
                </mrow>
            </msup>
        </mrow>
        <mrow>
            <mspace width="3em"/>
            <mtext>(6)</mtext>
        </mrow>
    </math>
</figure>
<p>
A binomial experiment has four properties:
</p>
<ol>
    <li>
        it consists of a sequence of <var>n</var> identical trials;
    </li>
    <li>
        two outcomes, success or failure, are possible on each trial;
    </li>
    <li>
        the probability of success on any trial, denoted <var>p</var>, does not change from trial to trial; and
    </li>
    <li>
        the trials are independent.
    </li>
</ol>
<p>
For instance, suppose that it is known that 10 percent of the owners of two-year old automobiles have had problems with their automobile’s electrical system. To compute the probability of finding exactly 2 owners that have had electrical system problems out of a group of 10 owners, the binomial probability mass function can be used by setting <var>n</var> = 10, <var>x</var> = 2, and <var>p</var> = 0.1 in equation 6; for this case, the probability is 0.1937.
</p>

<h3>The Poisson distribution</h3>
<p>
The <strong>Poisson</strong> probability distribution is often used as a model of the number of arrivals at a facility within a given period of time. For instance, a random variable might be defined as the number of telephone calls coming into an airline reservation system during a period of 15 minutes. If the mean number of arrivals during a 15-minute interval is known, the Poisson probability mass function given by equation 7 can be used to compute the probability of <var>x</var> arrivals.
</p>
<figure>
    <math displaystyle="true">
        <mi>f</mi>
        <mo>&ApplyFunction;</mo>
        <mo fence="true">&lpar;</mo>
        <mi>x</mi>
        <mo fence="true">&rpar;</mo>
        <mo>&equals;</mo>
        <mrow>
            <mfrac>
                <mrow>
                    <msup>
                        <mi>&mu;</mi>
                        <mi>x</mi>
                    </msup>
                    <mo>&InvisibleTimes;</mo>
                    <msup>
                        <mi>&escr;</mi>
                        <mrow>
                            <mo>&minus;</mo>
                            <mi>&mu;</mi>
                        </mrow>
                    </msup>
                </mrow>
                <mrow>
                    <mi>x</mi>
                    <mo>&excl;</mo>
                </mrow>
            </mfrac>
        </mrow>
        <mrow>
            <mspace width="3em"/>
            <mtext>(7)</mtext>
        </mrow>
    </math>
</figure>

</section>

<!-- ******************************************************************** -->
<h1>Estimation</h1>
<section id="estimation" class="description">

<p>
It is often of interest to learn about the characteristics of a large group of elements such as individuals, households, buildings, products, parts, customers, and so on. All the elements of interest in a particular study form the population. Because of time, cost, and other considerations, data often cannot be collected from every element of the population. In such cases, a subset of the population, called a <strong>sample</strong>, is used to provide the data. Data from the sample are then used to develop <strong>estimates</strong> of the characteristics of the larger <strong>population</strong>. The process of using a sample to make inferences about a population is called <strong>statistical inference</strong>.
</p>
<p>
Characteristics such as the population mean, the population variance, and the population proportion are called <strong>parameters</strong> of the population. Characteristics of the sample such as the sample mean, the sample variance, and the sample proportion are called <strong>sample statistics</strong>. There are two types of estimates: point and interval. A <strong>point estimate</strong> is a value of a sample statistic that is used as a <em>single estimate</em> of a population parameter. <em>No statements</em> are made about the quality or precision of a point estimate. Statisticians prefer <strong>interval estimates</strong> because interval estimates are accompanied by a statement concerning the <em>degree of confidence</em> that the interval contains the population parameter being estimated. Interval estimates of population parameters are called <strong>confidence intervals</strong>.
</p>

<!-- ******************************************************************** -->
<h2>Sampling and sampling distributions</h2>
<p>
Although sample survey methods will be discussed in more detail below in the section Sample survey methods, it should be noted here that the methods of statistical inference, and estimation in particular, are based on the notion that a probability sample has been taken. The key characteristic of a <strong>probability sample</strong> is that each element in the population has a known probability of being included in the sample. The most fundamental type is a simple random sample.
</p>
<p>
For a population of size <var>N</var>, a simple random sample is a sample selected such that each possible sample of size <var>n</var> has the same probability of being selected. Choosing the elements from the population one at a time so that each element has the same probability of being selected will provide a simple random sample. Tables of <strong>random numbers</strong>, or computer-generated random numbers, can be used to guarantee that each element has the same probability of being selected.
</p>
<p>
A sampling distribution is a probability distribution for a sample statistic. Knowledge of the sampling distribution is necessary for the construction of an interval estimate for a population parameter. This is why a probability sample is needed; without a probability sample, the sampling distribution cannot be determined and an interval estimate of a parameter cannot be constructed.
</p>

<h2>Estimation of a population mean</h2>
<p>
The most fundamental point and interval estimation process involves the estimation of a <strong>population mean</strong>. Suppose it is of interest to estimate the population mean, <var>&mu;</var>, for a quantitative variable. Data collected from a simple random sample can be used to compute the <strong>sample mean</strong>,
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>,
where the value of
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>
provides a <em>point estimate</em> of <var>&mu;</var>.
</p>
<p>
When the sample mean is used as a point estimate of the population mean, some <strong>error</strong> can be expected owing to the fact that a sample, or subset of the population, is used to compute the point estimate. The absolute value of the difference between the sample mean,
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>,
and the population mean, <var>&mu;</var>, written
    <math>
        <mrow>
            <mo fence="true">&verbar;</mo>
            <mover>
                <mi>x</mi>
                <mo>&OverBar;</mo>
            </mover>
            <mo>&minus;</mo>
            <mi>&mu;</mi>
            <mo fence="true">&verbar;</mo>
        </mrow>
    </math>,
is called the <strong>sampling error</strong>. Interval estimation incorporates a probability statement about the magnitude of the sampling error. The sampling distribution of
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>
provides the basis for such a statement.
</p>
<p>
Statisticians have shown that the mean of the sampling distribution of
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>
is equal to the population mean, <var>&mu;</var>, and that the <strong>standard deviation</strong> is given by
    <math>
        <mi>&sigma;</mi>
        <mo>&frasl;</mo>
        <msqrt>
            <mi>n</mi>
        </sqrt>
    </math>,
where <var>&sigma;</var> is the <strong>population standard deviation</strong>. The standard deviation of a sampling distribution is called the <strong>standard error</strong>. For large sample sizes, the <strong>central limit theorem</strong> indicates that the sampling distribution of
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>
can be approximated by a normal probability distribution. As a matter of practice, statisticians usually consider samples of size 30 or more to be large.
</p>
<p>
In the large-sample case, a 95% confidence interval estimate for the population mean is given by
    <math>
        <mrow>
            <mover>
                <mi>x</mi>
                <mo>&OverBar;</mo>
            </mover>
            <mo>&PlusMinus;</mo>
            <mrow>
                <mn>1.96</mn>
                <mo>&InvisibleTimes;</mo>
                <mi>&sigma;</mi>
            </mrow>
            <mo>&frasl;</mo>
            <msqrt>
                <mi>n</mi>
            </msqrt>
        </mrow>
    </math>.
When the population standard deviation, <var>&sigma;</var>, is unknown, the sample standard deviation is used to estimate <var>&sigma;</var> in the confidence interval formula. The quantity
    <math>
        <mrow>
            <mn>1.96</mn>
            <mo>&InvisibleTimes;</mo>
            <mi>&sigma;</mi>
        </mrow>
        <mo>&frasl;</mo>
        <msqrt>
            <mi>n</mi>
        </msqrt>
    </math>
is often called the <strong>margin of error</strong> for the estimate. The quantity
    <math>
        <mi>&sigma;</mi>
        <mo>&frasl;</mo>
        <msqrt>
            <mi>n</mi>
        </msqrt>
    </math>
is the <strong>standard error</strong>, and 1.96 is the number of standard errors from the mean necessary to include 95% of the values in a normal distribution. The interpretation of a 95% confidence interval is that 95% of the intervals constructed in this manner will contain the population mean. Thus, any interval computed in this manner has a 95% confidence of containing the population mean. By changing the constant from 1.96 to <em>1.645, a 90% confidence interval</em> can be obtained. It should be noted from the formula for an interval estimate that a 90% confidence interval is narrower than a 95% confidence interval and as such has a slightly smaller confidence of including the population mean. Lower levels of confidence lead to even more narrow intervals. In practice, a 95% confidence interval is the most widely used.
</p>
<p>
Owing to the presence of the
    <math>
        <msqrt>
            <mi>n</mi>
        </msqrt>
    </math>
term in the formula for an interval estimate, the sample size affects the margin of error. Larger sample sizes lead to smaller margins of error. This observation forms the basis for procedures used to select the sample size. Sample sizes can be chosen such that the confidence interval satisfies any desired requirements about the size of the margin of error.
</p>
<p>
The procedure just described for developing interval estimates of a population mean is based on the use of a large sample. In the small-sample case &mdash; i.e., where the sample size <var>n</var> is less than 30 &mdash; the <strong><i>t</i>-distribution</strong> is used when specifying the margin of error and constructing a confidence interval estimate. For example, at a 95% level of confidence, a value from the <i>t</i>-distribution, determined by the value of <var>n</var>, would replace the 1.96 value obtained from the normal distribution. The <i>t</i>-values will always be larger, leading to wider confidence intervals, but, as the sample size becomes larger, the <i>t</i>-values get closer to the corresponding values from a normal distribution. With a sample size of 25, the <i>t</i>-value used would be 2.064, as compared with the normal probability distribution value of 1.96 in the large-sample case.
</p>

<h3>Estimation of other parameters</h3>
<p>
For qualitative variables, the <strong>population proportion</strong> is a parameter of interest. A point estimate of the population proportion is given by the <strong>sample proportion</strong>. With knowledge of the sampling distribution of the sample proportion, an interval estimate of a population proportion is obtained in much the same fashion as for a population mean. Point and interval estimation procedures such as these can be applied to other population parameters as well. For instance, interval estimation of a <strong>population variance</strong>, standard deviation, and total can be required in other applications.
</p>

<h3>Estimation procedures for two populations</h3>
<p>
The estimation procedures can be extended to two populations for comparative studies. For example, suppose a study is being conducted to determine differences between the salaries paid to <em>a population of men</em> and <em>a population of women</em>. Two independent simple random samples, one from the population of men and one from the population of women, would provide two sample means,
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
        <mn>₁</mn>
    </math>
and
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
        <mn>₂</mn>
    </math>.
The difference between the two sample means,
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
        <mn>₁</mn>
        <mo>&minus;</mo>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
        <mn>₂</mn>
    </math>,
would be used as a point estimate of the difference between the two population means. The sampling distribution of
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
        <mn>₁</mn>
        <mo>&minus;</mo>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
        <mn>₂</mn>
    </math>
would provide the basis for a confidence interval estimate of the difference between the two population means. For qualitative variables, point and interval estimates of the difference between population proportions can be constructed by considering the difference between sample proportions.
</p>

</section>

<!-- ******************************************************************** -->
<h1>Hypothesis testing</h1>
<section id="hypothesis_testing" class="description">

<p>
Hypothesis testing is a form of statistical inference that uses data from a sample to draw conclusions about a population parameter or a population probability distribution. First, a tentative assumption is made about the parameter or distribution. This assumption is called the <strong>null hypothesis</strong> and is denoted by <var>H₀</var>. An <strong>alternative hypothesis</strong> (denoted <var>H<sub>a</sub></var>), which is the opposite of what is stated in the null hypothesis, is then defined. The hypothesis-testing procedure involves using sample data to determine whether or not <var>H₀</var> can be rejected. If <var>H₀</var> is rejected, the statistical conclusion is that the alternative hypothesis <var>H<sub>a</sub></var> is true.
</p>
<p>
For example, assume that a radio station selects the music it plays based on the assumption that the average age of its listening audience is 30 years. To determine whether this assumption is valid, a hypothesis test could be conducted with the null hypothesis given as <var>H₀</var>: <var>&mu;</var> = 30 and the alternative hypothesis given as <var>H<sub>a</sub></var>: <var>&mu;</var> &ne; 30. Based on a sample of individuals from the listening audience, the sample mean age,
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>,
can be computed and used to determine whether there is sufficient statistical evidence to reject <var>H₀</var>. Conceptually, a value of the sample mean that is “close” to 30 is consistent with the null hypothesis, while a value of the sample mean that is “not close” to 30 provides support for the alternative hypothesis. What is considered “close” and “not close” is determined by using the sampling distribution of
    <math>
        <mover>
            <mi>x</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>.
</p>
<p>
Ideally, the hypothesis-testing procedure leads to the acceptance of H₀ when H₀ is true and the rejection of <var>H₀</var> when <var>H₀</var> is false. Unfortunately, since hypothesis tests are based on sample information, the possibility of errors must be considered. A <strong>type I error</strong> corresponds to rejecting <var>H₀</var> when <var>H₀</var> is actually true, and a <strong>type II error</strong> corresponds to accepting <var>H₀</var> when <var>H₀</var> is false. The probability of making a type I error is denoted by <var>&alpha;</var>, and the probability of making a type II error is denoted by <var>&beta;</var>.
</p>
<p>
In using the hypothesis-testing procedure to determine if the null hypothesis should be rejected, the person conducting the hypothesis test specifies the maximum allowable probability of making a type I error, called the <strong>level of significance</strong> for the test. Common choices for the level of significance are <var>&alpha;</var> = 0.05 and <var>&alpha;</var> = 0.01. Although most applications of hypothesis testing control the probability of making a type I error, they do not always control the probability of making a type II error. A graph known as an operating-characteristic curve can be constructed to show how changes in the sample size affect the probability of making a type II error.
</p>
<p>
A concept known as the <strong><var>p</var>-value</strong> provides a convenient basis for drawing conclusions in hypothesis-testing applications. The <var>p</var>-value is a measure of how likely the sample results are, assuming the null hypothesis is true; the smaller the <var>p</var>-value, the less likely the sample results. If the <var>p</var>-value is less than <var>&alpha;</var>, the null hypothesis can be rejected; otherwise, the null hypothesis cannot be rejected. The <var>p</var>-value is often called the observed level of significance for the test.
</p>
<p>
A hypothesis test can be performed on parameters of one or more populations as well as in a variety of other situations. In each instance, the process begins with the formulation of null and alternative hypotheses about the population. In addition to the population mean, hypothesis-testing procedures are available for <em>population parameters</em> such as proportions, variances, standard deviations, and medians.
</p>
<p>
Hypothesis tests are also conducted in regression and correlation analysis to determine if the regression relationship and the correlation coefficient are statistically significant (see below Regression and correlation analysis). A goodness-of-fit test refers to a hypothesis test in which the null hypothesis is that the population has a specific probability distribution, such as a normal probability distribution. Nonparametric statistical methods also involve a variety of hypothesis-testing procedures.
</p>

</section>

<!-- ******************************************************************** -->
<h1>Bayesian methods</h1>
<section id="bayesian_methods" class="description">

<p>
The methods of statistical inference previously described are often referred to as <em>classical</em> methods. Bayesian methods (so called after the English mathematician Thomas Bayes) provide alternatives that allow one to combine prior information about a population parameter with information contained in a sample to guide the statistical inference process. A <strong>prior probability distribution</strong> for a parameter of interest is specified first. Sample information is then obtained and combined through an application of <strong>Bayes’s theorem</strong> to provide a <strong>posterior probability distribution</strong> for the parameter. The posterior distribution provides the basis for statistical inferences concerning the parameter.
</p>
<p>
A key, and somewhat <em>controversial</em>, feature of Bayesian methods is the notion of a probability distribution for a population parameter. According to classical statistics, parameters are constants and cannot be represented as random variables. Bayesian proponents argue that, if a parameter value is unknown, then it makes sense to specify a probability distribution that describes the possible values for the parameter as well as their likelihood. The Bayesian approach permits the use of objective data or subjective opinion in specifying a prior distribution. With the Bayesian approach, different individuals might specify different prior distributions. Classical statisticians argue that for this reason Bayesian methods suffer from a lack of objectivity. Bayesian proponents argue that the classical methods of statistical inference have built-in subjectivity (through the choice of a sampling plan) and that the advantage of the Bayesian approach is that the subjectivity is made explicit.
</p>
<p>
Bayesian methods have been used extensively in statistical decision theory (see below Decision analysis). In this context, Bayes’s theorem provides a mechanism for combining a prior probability distribution for the states of nature with sample information to provide a revised (posterior) probability distribution about the states of nature. These posterior probabilities are then used to make better decisions.
</p>

</section>

<!-- ******************************************************************** -->
<h1>Experimental design</h1>
<section id="experimental_design" class="description">

<p>
Data for statistical studies are obtained by conducting either experiments or surveys. Experimental design is the branch of statistics that deals with the design and analysis of experiments. The methods of experimental design are widely used in the fields of agriculture, medicine, biology, marketing research, and industrial production.
</p>
<p>
In an experimental study, variables of interest are identified. One or more of these variables, referred to as the factors of the study, are controlled so that data may be obtained about how the factors influence another variable referred to as the response variable, or simply the response. As a case in point, consider an experiment designed to determine the effect of three different exercise programs on the cholesterol level of patients with elevated cholesterol. Each patient is referred to as an experimental unit, the response variable is the cholesterol level of the patient at the completion of the program, and the exercise program is the factor whose effect on cholesterol level is being investigated. Each of the three exercise programs is referred to as a treatment.
</p>
<p>
Three of the more widely used experimental designs are
<ul>
    <li>the completely randomized design,</li>
    <li>the randomized block design, and</li>
    <li>the factorial design.</li>
</ul>
In a completely randomized experimental design, the treatments are randomly assigned to the experimental units. For instance, applying this design method to the cholesterol-level study, the three types of exercise program (treatment) would be randomly assigned to the experimental units (patients).
</p>
<p>
The use of a completely randomized design will yield less precise results when <strong>factors</strong> not accounted for by the experimenter affect the response variable. Consider, for example, an experiment designed to study the effect of two different gasoline additives on the fuel efficiency, measured in miles per gallon (mpg), of full-size automobiles produced by three manufacturers. Suppose that 30 automobiles, 10 from each manufacturer, were available for the experiment. In a completely randomized design the two gasoline additives (treatments) would be randomly assigned to the 30 automobiles, with each additive being assigned to 15 different cars. Suppose that manufacturer 1 has developed an engine that gives its full-size cars a higher fuel efficiency than those produced by manufacturers 2 and 3. A completely randomized design could, by chance, assign gasoline additive 1 to a larger proportion of cars from manufacturer 1. In such a case, gasoline additive 1 might be judged to be more fuel efficient when <em>in fact</em> the difference observed is actually due to the better engine design of automobiles produced by manufacturer 1. To prevent this from occurring, a statistician could design an experiment in which both gasoline additives are tested using five cars produced by each manufacturer; in this way, any effects due to the manufacturer would not affect the test for significant differences due to gasoline additive. In this revised experiment, each of the manufacturers is referred to as a block, and the experiment is called a randomized block design. In general, blocking is used in order to enable comparisons among the treatments to be made within blocks of homogeneous experimental units.
</p>
<p>
Factorial experiments are designed to draw conclusions about more than one factor, or variable. The term factorial is used to indicate that all possible combinations of the factors are considered. For instance, if there are two factors with a levels for factor 1 and b levels for factor 2, the experiment will involve collecting data on ab treatment combinations. The <em>factorial design</em> can be extended to experiments involving more than two factors and experiments involving <em>partial factorial designs</em>.
</p>

</section>

<!-- ******************************************************************** -->
<h2>Analysis of variance and significance testing</h2>
<section id="variance_analysis" class="description">

<p>
A computational procedure frequently used to analyze the data from an experimental study employs a statistical procedure known as the <strong>analysis of variance</strong>. For a single-factor experiment, this procedure uses a hypothesis test concerning equality of treatment means to determine if the factor has a statistically significant effect on the response variable. For experimental designs involving multiple factors, a test for the significance of each individual factor as well as interaction effects caused by one or more factors acting jointly can be made. Further discussion of the analysis of variance procedure is contained in the subsequent section.
</p>

</section>

<!-- ******************************************************************** -->
<h2>Regression and correlation analysis</h2>
<section id="correlation_analysis" class="description">

<p>
<strong>Regression analysis</strong> involves identifying the relationship between a <em>dependent variable</em> and one or more <strong>independent variables</strong>. A model of the relationship is hypothesized, and estimates of the parameter values are used to develop an <strong>estimated regression equation</strong>. Various tests are then employed to determine if the model is satisfactory. If the model is deemed satisfactory, the estimated regression equation can be used to predict the value of the dependent variable given values for the independent variables.
</p>

<h3>Regression model</h3>
<p>
In <strong>simple linear regression</strong>, the model used to describe the relationship between a single dependent variable <var>y</var> and a single independent variable <var>x</var> is
<figure>
    <math>
        <mi>y</mi>
        <mo>&equals;</mo>
        <mi>&beta;₀</mi>
        <mo>&plus;</mo>
        <mi>&beta;₁</mi>
        <mo>&InvisibleTimes;</mo>
        <mi>x</mi>
        <mo>&plus;</mo>
        <mi>&epsilon;</mi>
    </math>
</figure>
<p>
<var>&beta;₀</var> and <var>&beta;₁</var> are referred to as the model parameters, and <var>&epsilon;</var> is a probabilistic error term that accounts for the variability in <var>y</var> that cannot be explained by the linear relationship with <var>x</var>. If the error term were not present, the model would be deterministic; in that case, knowledge of the value of <var>x</var> would be sufficient to determine the value of <var>y</var>.
</p>
<p>
In <strong>multiple regression analysis</strong>, the model for simple linear regression is extended to account for the relationship between the dependent variable <var>y</var> and <var>p</var> independent variables <var>x₁</var>, <var>x₂</var>, &hellip;, <var>x<sub>p</sub></var>. The general form of the multiple regression model is
<figure>
    <math>
        <mi>y</mi>
        <mo>&equals;</mo>
        <mi>&beta;₀</mi>
        <mo>&plus;</mo>
        <mi>&beta;₁</mi>
        <mo>&InvisibleTimes;</mo>
        <mi>x₁</mi>
        <mo>&plus;</mo>
        <mi>&beta;₂</mi>
        <mo>&InvisibleTimes;</mo>
        <mi>x₂</mi>
        <mo>&plus;</mo>
        <mi>&hellip;</mi>
        <mo>&plus;</mo>
        <msub>
            <mi>&beta;</mi>
            <mi>p</mi>
        </msub>
        <mo>&InvisibleTimes;</mo>
        <msub>
            <mi>x</mi>
            <mi>p</mi>
        </msub>
        <mo>&plus;</mo>
        <mi>&epsilon;</mi>
    </math>
</figure>
The parameters of the model are the <var>&beta;₀</var>, <var>&beta;₁</var>, &hellip;, <var>&beta;<sub>p</sub></var>, and <var>&epsilon;</var> is the error term.
</p>

<h3>Least squares method</h3>
<p>
Either a simple or multiple regression model is initially posed as a hypothesis concerning the relationship among the dependent and independent variables. The <strong>least squares method</strong> is the most widely used procedure for developing estimates of the model parameters. For simple linear regression, the least squares estimates of the model parameters <var>&beta;₀</var> and <var>&beta;₁</var> are denoted <var>b₀</var> and <var>b₁</var>. Using these estimates, an estimated regression equation is constructed:
    <math>
        <mrow>
            <mover accent="true">
                <mi>y</mi>
                <mo>&Hat;</mo>
            </mover>
            <mo>&equals;</mo>
            <mi>b₀</mi>
            <mo>&plus;</mo>
            <mi>b₁</mi>
            <mo>&InvisibleTimes;</mo>
            <mi>x</mi>
        </mrow>
    </math>.
The graph of the estimated regression equation for simple linear regression is a straight line approximation to the relationship between <var>y</var> and <var>x</var>.
</p>





</body>
</html>

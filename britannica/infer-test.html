<!DOCTYPE html>
<html lang="ja">
<head>
    <title>Inference and Test</title>
    <link rel="stylesheet" href="dark.css" type="text/css">
    <link rel="stylesheet" href="indent.css" type="text/css">
</head>
<body>

<h1>Inference and Test</h1>

<section id="estimate_test" class="description">

<h3>統計的推定と統計的仮説検定 [統計局:学習教材1]</h3>
<p>
統計的推定とは、母集団の特性値（平均や分散など）を標本のデータから統計学的に推測することを示します。また、統計的仮説検定とは、母集団に関するある仮説が統計的に成り立つか否かを、標本のデータを用いて判断することです。
</p>

<h3>検定と推定の関係 [青木:用語辞典II]</h3>
<p>
統計的仮説検定と区間推定はともに，ある母数を持つ母集団から標本を抽出し，その標本から得られる統計量を対象とするという類似点を持つ。しかし，両者は推論の方向が逆の関係にあると言ってよい。
</p>
<ul>
    <li>
        <em>区間推定</em>では，統計量の分布を元に，特定の標本統計量の値がある確率の下で起こり得る母数の存在する<em>区間を求める</em>。
    </li>
    <li>
        <em>仮説検定</em>では，母数に関するある特定の仮定（<em>帰無仮説</em>）を設定し，その下で，ある特定の統計量が得られる<em>確率を求める</em>。 もし，得られた確率がある基準（<em>有意水準</em>）より小さければ，母数に関する仮定が誤っている可能性が強いと判断すればよい。
    </li>
</ul>

</section>

<!-- ******************************************************************** -->
<h2>Inference</h2>

<section id="inference" class="description">

<h3>推定 statistical inference [青木:用語辞典I,II]</h3>
<p>
より正確にいえば<strong>統計的推定</strong>。標本から得られた統計量をもとにして，母数の存在する範囲を求める手法。例えば，「標本平均から母平均の存在範囲を知りたい」，「標本比率から母比率の存在範囲を知りたい」などの場合に用いる。
</p>
<p>
統計学的推測とは，標本から母集団の統計学的な性質を推測することである。その目的の一つに<em>推定</em>と呼ばれるものがある。例えば，無作為に選ばれた数校で身長を測定したとき，標本平均
    <math>
        <mover>
            <mi>X</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>
は県下の全小学6年生の母平均
    <math><mi>&mu;</mi></math>
のよい推定値を与えるであろう。推定値を求める方法は2通りある。
</p>
<ul>
    <li>
        <strong>点推定法</strong>:
        ある1つの数値（点推定値）として母数を推定する
    </li>
    <li>
        <strong>区間推定法</strong>:
        ある確からしさのもとで母数の存在する区間を推定する（区間推定値）
    </li>
</ul>

<h3>Statistical Inference [STEPS:Glossary]</h3>
<p>
<strong>Statistical Inference</strong> makes use of information from a sample to draw conclusions (inferences) about the population from which the sample was taken.
</p>

<h3>確率標本 probability sample [青木:用語辞典II]</h3>
<p>
標本を抽出する際に，母集団の代表と思われる標本を主観的に選択抽出する有意抽出法が使われていた時代がある。しかし，このようにして得られた標本には偏り（バイアス）があり，誤差の評価において統計学的な取り扱いが不可能となることがある。
</p>
<p>
それに代わるものとして，無作為抽出法がある。母集団から抽出された標本が次の条件を満たすとき，この標本を<strong>確率標本</strong>という（無作為標本，あるいは，ランダム・サンプルともいう）。
</p>
<ul>
    <li>
        母集団からある個体を選ぶとき，その母集団に属する全ての個体にとって選ばれる確率が等しいこと。
    </li>
    <li>
        それぞれの個体が独立に抽出されること。
    </li>
</ul>

<h3>Random Sampling [STEPS:Glossary]</h3>
<p>
Random sampling is a sampling technique where we select a group of subjects (a sample) for study from a larger group (a population). Each individual is chosen entirely by chance and each member of the population has a known, but possibly non-equal, chance of being included in the sample.
By using random sampling, the likelihood of bias is reduced.
</p>

<h3>点推定値 point estimate [青木:自習ノート]</h3>
<table>
    <tr>
        <th>点推定値</th>
        <th>標本統計量</th>
    </tr>
    <tr>
        <td>母平均
            <math><mi>&mu;</mi></math>
        </td>
        <td>平均値 mean;
            最頻値 mode;
            中央値 median
        </td>
    </tr>
    <tr>
        <td>母分散
            <math>
                <msup>
                    <mi>&sigma;</mi>
                    <mn>2</mn>
                </msup>
            </math>
        </td>
        <td>分散
            <math><mi>V</mi></math>
        </td>
    </tr>
    <tr>
        <td>母比率
            <math><mi>p</mi></math>
        </td>
        <td>比率
            <math>
                <mi>m</mi>
                <mo>&frasl;</mo>
                <mi>n</mi>
            </math>
        </td>
    </tr>
    <tr>
        <td>母相関係数
            <math><mi>&rho;</mi></math>
        </td>
        <td>相関係数: ピアソン, ケンドール, スピアマン
        </td>
    </tr>
</table>

<h3>Estimate [STEPS:Glossary]</h3>
<p>
An estimate is an indication of the value of an unknown quantity based on observed data.

More formally, an estimate is the particular value of an <i>estimator</i> that is obtained from a particular <u>sample</u> of data and used to indicate the value of a <u>parameter</u>.
</p>

<h3>Estimator [STEPS:Glossary]</h3>
<p>
An estimator is any quantity calculated from the <u>sample</u> data which is used to give information about an unknown quantity in the <u>population</u>. For example, the sample mean is an estimator of the population mean.

Estimators of population parameters are sometimes distinguished from the true value by using the symbol 'hat'. For example,
</p>
<ul>
    <li>
        <math>
            <mi>&sigma;</mi>
            <mo>&equals;</mo>
            <mtext>true population standard deviation</mtext>
        </math>
    </li>
    <li>
        <math>
            <mover>
                <mi>&sigma;</mi>
                <mo>&Hat;</mo>
            </mover>
            <mo>&equals;</mo>
            <mtext>
                estimated (from a sample) population standard deviation
            </mtext>
        </math>
    </li>
</ul>

<h3>Estimation [STEPS:Glossary]</h3>
<p>
Estimation is the process by which sample data are used to indicate the value of an unknown quantity in a population.

Results of estimation can be expressed as
</p>
<ul>
    <li>
        a single value, known as a <em>point estimate</em>, or
    </li>
    <li>
        a range of values, known as a <em>confidence interval</em>.
    </li>
</ul>

<h3>区間推定 interval estimation [統計局:学習教材1]</h3>
<p>
<strong>区間推定</strong>は、母集団が正規分布に従うと仮定できる場合に、標本のデータを用いて母平均などの推定量を、１つの値ではなく、入る区間（幅）で推定します。推定する区間を<em>信頼区間</em>と呼び、「90%信頼区間」「95%信頼区間」「99%信頼区間」などで求めます。
</p>
<p>
例えば「95%信頼区間」で求めた場合、「母集団から標本をとりだし、その標本から母平均の95%信頼区間を求める」ことを100回実施したとき、「95回程度はその区間内に母平均が入る」ことを表します。（母平均は知られていないだけで確定した値なので、得られた標本のもとで母平均がその区間内にある確率が95%という意味ではないことに注意してください。）
</p>

<h3>信頼区間 confidence interval [青木:用語辞典I]</h3>
<p>
<var>&Theta;</var> を推定したい母数，
    <math>
        <mrow>
            <mi>&alpha;</mi>
            <mspace width="0.5em"/>
            <mo fence="true">&lpar;</mo>
            <mn>0</mn>
            <mo>&lt;</mo>
            <mi>&alpha;</mi>
            <mo>&lt;</mo>
            <mn>1</mn>
            <mo fence="true">&rpar;</mo>
        </mrow>
    </math>
を一定の確率としたとき，
    <math>
        <mrow>
            <msub>
                <mi>P</mi>
                <mi>r</mi>
            </msub>
            <mo fence="true">&lbrace;</mo>
            <mi>t₁</mi>
            <mo>&le;</mo>
            <mi>&Theta;</mi>
            <mo>&le;</mo>
            <mi>t₂</mi>
            <mo fence="true">&rbrace;</mo>
            <mo>&equals;</mo>
            <mn>1</mn>
            <mo>&minus;</mo>
            <mi>&alpha;</mi>
        </mrow>
    </math>
となるような二つの統計量 <var>t₁</var> と <var>t₂</var> を求めることができるならば， <var>t₁</var> と <var>t₂</var> が挟む区間を<strong>信頼区間</strong>と呼ぶ。このような推定が繰り返し行われたとき，<var>&Theta;</var> が計算された信頼区間内に含まれるのはその繰り返しの中の
    <math>
        <mrow>
            <mn>100</mn>
            <mo>&InvisibleTimes;</mo>
            <mo fence="true">&lpar;</mo>
            <mn>1</mn>
            <mo>&minus;</mo>
            <mi>&alpha;</mi>
            <mo fence="true">&rpar;</mo>
            <mo>%</mo>
        </mrow>
    </math>
である。
</p>

<h3>信頼度 confidence level [青木:用語辞典I]</h3>
<p>
信頼係数とも呼ぶ。<var>&Theta;</var> を推定したい母数，
    <math>
        <mrow>
            <mi>&alpha;</mi>
            <mspace width="0.5em"/>
            <mo fence="true">&lpar;</mo>
            <mn>0</mn>
            <mo>&lt;</mo>
            <mi>&alpha;</mi>
            <mo>&lt;</mo>
            <mn>1</mn>
            <mo fence="true">&rpar;</mo>
        </mrow>
    </math>
を一定の確率としたとき，
    <math>
        <mrow>
            <msub>
                <mi>P</mi>
                <mi>r</mi>
            </msub>
            <mo fence="true">&lbrace;</mo>
            <mi>t₁</mi>
            <mo>&le;</mo>
            <mi>&Theta;</mi>
            <mo>&le;</mo>
            <mi>t₂</mi>
            <mo fence="true">&rbrace;</mo>
            <mo>&equals;</mo>
            <mn>1</mn>
            <mo>&minus;</mo>
            <mi>&alpha;</mi>
        </mrow>
    </math>
となるような二つの統計量 <var>t₁</var> と <var>t₂</var> が信頼区間を構成する。
このときの
    <math>
        <mrow>
            <mn>1</mn>
            <mo>&minus;</mo>
            <mi>&alpha;</mi>
        </mrow>
    </math>
をパーセント表示して，
    <math>
        <mrow>
            <mn>100</mn>
            <mo>&InvisibleTimes;</mo>
            <mo fence="true">&lpar;</mo>
            <mn>1</mn>
            <mo>&minus;</mo>
            <mi>&alpha;</mi>
            <mo fence="true">&rpar;</mo>
            <mo>%</mo>
        </mrow>
    </math>
のことを<strong>信頼度</strong>という。
</p>

<h3>Confidence Interval [STEPS:Glossary]</h3>
<p>
A <strong>confidence interval</strong> gives an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a given set of sample data.
</p>
<p>
If independent samples are taken repeatedly from the same population, and a confidence interval calculated for each sample, then a certain percentage (<em>confidence level</em>) of the intervals will include the unknown population parameter. Confidence intervals are usually calculated so that this percentage is 95%, but we can produce 90%, 99%, 99.9% (or whatever) confidence intervals for the unknown parameter.
</p>
<p>
The width of the confidence interval gives us some idea about how uncertain we are about the unknown parameter (see precision). A very wide interval may indicate that more data should be collected before anything very definite can be said about the parameter.
</p>
<p>
Confidence intervals are more informative than the simple results of hypothesis tests (where we decide "reject <var>H₀</var>" or "don't reject <var>H₀</var>") since they provide a range of plausible values for the unknown parameter.
</p>

<h3>Confidence Level (STEPS:Glossary)</h3>
<p>
The <strong>confidence level</strong> is the probability value
    <math>
        <mo fence="true">&lpar;</mo>
        <mn>1</mn>
        <mo>&minus;</mo>
        <mi>&alpha;</mi>
        <mo fence="true">&rpar;</mo>
    </math>
associated with a confidence interval.
It is often expressed as a percentage. For example, say
    <math>
        <mi>&alpha;</mi>
        <mo>&equals;</mo>
        <mn>0.05</mn>
        <mo>&equals;</mo>
        <mn>5%</mn>
    </math>
then the confidence level is equal to
    <math>
        <mo fence="true">&lpar;</mo>
        <mn>1</mn>
        <mo>&minus;</mo>
        <mn>0.05</mn>
        <mo fence="true">&rpar;</mo>
        <mo>&equals;</mo>
        <mn>0.95</mn>
    </math>,
i.e. a 95% confidence level.
</p>
<p>
Example:
Suppose an opinion poll predicted that, if the election were held today, the Conservative party would win 60% of the vote. The pollster might attach a 95% confidence level to the interval 60% plus or minus 3%. That is, he thinks it very likely that the Conservative party would get between 57% and 63% of the total vote.
</p>

<h3>信頼限界 confidence limit [青木:用語辞典I]</h3>
<p>
信頼区間の上限と下限。
</p>

<h3>Confidence Limits (STEPS:Glossary)</h3>
<p>
<strong>Confidence limits</strong> are the lower and upper boundaries / values of a confidence interval, that is, the values which define the range of a confidence interval.

The upper and lower bounds of a 95% confidence interval are the 95% confidence limits. These limits may be taken for other confidence levels, for example, 90%, 99%, 99.9%.
</p>

<h3>標準誤差 standard error [青木:用語辞典I]</h3>
<p>
標本平均値，標本分散などのような，標本から得られる標本統計量も<em>散らばり</em>を持つ。これら標本統計量の標準偏差を，特に<strong>標準誤差</strong>と呼ぶ。例えば，母平均 <var>&mu;</var>，母分散 <var>&sigma;<sup>2</sup></var> の正規母集団から <var>n</var> ケース取り出したときの標本平均
    <math>
        <mover>
            <mi>X</mi>
            <mo>&OverBar;</mo>
        </mover>
    </math>
は，平均が <var>&mu;</var>，標本分散が
    <math displaystyle="true">
        <mfrac>
            <msup>
                <mi>&sigma;</mi>
                <mn>2</mn>
            </msup>
            <mi>n</mi>
        </mfrac>
    </math>，
標準誤差が
    <math displaystyle="true">
        <mrow>
            <mi>S.E.</mi>
            <mo>&equals;</mo>
            <mfrac>
                <mi>&sigma;</mi>
                <msqrt>
                    <mi>n</mi>
                </msqrt>
            </mfrac>
        </mrow>
    </math>
の正規分布に従う。これから，
</p>
<figure>
    <math>
        <mrow>
            <mo fence="true">&verbar;</mo>
            <mover>
                <mi>X</mi>
                <mo>&OverBar;</mo>
            </mover>
            <mo>&minus;</mo>
            <mi>&mu;</mi>
            <mo fence="true">&verbar;</mo>
            <mo>&ge;</mo>
            <mn>1.96</mn>
            <mo>&times;</mo>
            <mi>S.E.</mi>
        </mrow>
    </math>
</figure>
<p>
となる標本が得られる確率は5%，母平均が
</p>
<figure>
    <math>
        <mrow>
            <mo fence="true">&lbrack;</mo>
            <mover>
                <mi>X</mi>
                <mo>&OverBar;</mo>
            </mover>
            <mo>&minus;</mo>
            <mn>1.96</mn>
            <mo>&InvisibleTimes;</mo>
            <mi>S.E.</mi>
            <mo separator="true">&comma;</mo>
            <mover>
                <mi>X</mi>
                <mo>&OverBar;</mo>
            </mover>
            <mo>&plus;</mo>
            <mn>1.96</mn>
            <mo>&InvisibleTimes;</mo>
            <mi>S.E.</mi>
            <mo fence="true">&rbrack;</mo>
        </mrow>
    </math>
</figure>
<p>
の区間に含まれる確率は95%などがわかる。前者は統計的検定，後者は統計的推測の例である。
</p>

<h3>Standard Error [STEPS:Glossary]</h3>
<p>
<strong>Standard error</strong> is the standard deviation of the values of a given function of the data (parameter), over all possible samples of the same size.
</p>

</section>

<!-- ******************************************************************** -->
<h2>Hypothesis test</h2>

<section id="hypothesis_testing" class="description">

<h3>帰無仮説と対立仮説 [青木:自習ノート]</h3>
<p>
研究者はある理論から予想される事実を証明するために，調査または実験を行い，データを収集する。
この際の作業仮説（または実験仮説）を<strong>対立仮説</strong> <var>H₁</var> とする。
例えば，“新しく開発された薬剤は，従来薬より有効である”とか，“男と女で読書時間に差がある”というのが対立仮説である。
</p>
<p>
これに対して，作業仮説を否定する仮説を<strong>帰無仮説</strong> <var>H₀</var> とする。
例えば，前の例では，“新しく開発された薬剤は，従来薬より有効とはいえない”とか，“男と女で読書時間に差はない”というのが帰無仮説である。
</p>
<p>
<strong>仮説検定</strong>の対象となるのは<em>帰無仮説</em>である。帰無仮説が棄却されれば対立仮説が支持されることになり，帰無仮説が棄却されて始めて研究者の調査・実験意図が達せられるわけで，この意味で 帰無仮説（無に帰される仮説）と呼ばれる。
</p>

<h3>仮説検定 hypothesis test [統計局:学習教材1]</h3>
<p>
検定は、母集団に関するある仮説が統計学的に成り立つか否かを、標本のデータを用いて判断することで、以下の1 &Tilde; 4の手順で実施します。例えば、「駅前のハンバーガー店のフライドポテトの重量が公表値のとおりか」を検証するため、<em>統計的仮説検定</em>を実施してみましょう。
</p>
<dl>
    <dt>1. 仮説を設定する</dt>
    <dd>
        仮説は、導き出したい結論とは反対の仮説を設定します。この仮説を<em>帰無仮説</em>と呼びます。
        <ul>
            <li>導き出したい結論: その重量が公表値のとおりではない。</li>
            <li>設定する帰無仮説: その重量が公表値のとおりである。</li>
        </ul>
    </dd>
    <dt>2. 有意水準を決定する</dt>
    <dd>
        <em>有意水準</em>とは、設定した帰無仮説が間違っていると判断する（帰無仮説を棄却する）確率のことです。有意水準0.05に設定した場合、5%以下の確率で生じる現象は、非常にまれなことであるとします。有意水準は、0.05や0.01が多く使われています。ここでは、有意水準0.05とします。
    </dd>
    <dt>3. 検証する</dt>
    <dd>
        設定した仮説が正しいとした場合、データから仮説が実現する確率を導いて、検証します。ただし、設定した仮説の値がどのくらいの確率で起こりうるのかが分からないため、検証するために使用する値「<em>検定統計量 <var>Z</var></em>」を算出して、検証します。（フライドポテトを10個購入し、その重量を計測した結果、
        <math>
            <mrow>
                <mi>Z</mi>
                <mo>&equals;</mo>
                <mo>&minus;</mo>
                <mn>2.64</mn>
            </mrow>
        </math>
        が得られたとします。）標準正規分布表から
        <math>
            <mrow>
                <mi>Z</mi>
                <mo>&equals;</mo>
                <mo>&minus;</mo>
                <mn>2.64</mn>
            </mrow>
        </math>
        に対応する値 0.0042 が得られ、両側検定では2倍にした値 0.0084 が <var>P</var> 値となります。先に決めた有意水準 0.05 よりも <var>P</var> 値 0.0084 が小さいことから、観察された事象が起こることは非常にまれなことであると判断できます。
    </dd>
    <dt>4. 背理法を用いて結論を導く</dt>
    <dd>
        検証の結果、帰無仮説は正しいとは言えないと分かります。よって、帰無仮説とは逆の結論である「その重量が公表値のとおりではない」が正しいと判断することができます。
    </dd>
</dl>
<p>
このように、仮説検定は、導き出したい結論とは逆の仮説を設定して、この仮説が正しいとは言えないことを証明することで、導き出したい結論が正しいとする方法であり、背理法の考え方に基づいています。
</p>

<h3>Hypothesis Test [STEPS:Glossary]</h3>
<p>
Setting up and testing hypotheses is an essential part of statistical inference. In order to formulate such a test, usually some theory has been put forward, either because it is believed to be true or because it is to be used as a basis for argument, but has not been proved, for example, claiming that a new drug is better than the current drug for treatment of the same symptoms.
</p>
<p>
In each problem considered, the question of interest is simplified into two competing claims / hypotheses between which we have a choice; the <strong>null hypothesis</strong>, denoted <var>H₀</var>, against the <strong>alternative hypothesis</strong>, denoted <var>H₁</var>.
</p>
<p>
The hypotheses are often <em>statements</em> about population parameters like expected value and variance; for example <var>H₀</var> might be that the expected value of the height of ten year old boys in the Scottish population is not different from that of ten year old girls.
</p>
<p>
The outcome of a hypothesis test is "Reject <var>H₀</var> in favour of <var>H₁</var>" or "Do not reject <var>H₀</var>". We never conclude "Reject <var>H₁</var>", or even "Accept <var>H₁</var>".
</p>
<p>
If we conclude "Do not reject <var>H₀</var>", this does not necessarily mean that the null hypothesis is true, it only suggests that there is not <em>sufficient evidence</em> against <var>H₀</var> in favour of <var>H₁</var>. Rejecting the null hypothesis then, suggests that the alternative hypothesis may be true.
</p>

<h3>有意確率 <var>p</var>-value [青木:用語辞典I]</h3>
<p>
統計的仮説検定において，帰無仮説のもとで得られた検定統計量が実現する確率。例えば，正規分布において標準得点が1.96以上となる確率は2.5%。<strong>有意確率</strong>がまえもって定めた<em>有意水準</em>より小さい場合に帰無仮説を棄却し，大きい場合に帰無仮説を採択する。
</p>

<h3>有意水準 significance level [青木:用語辞典I]</h3>
<p>
統計的仮説検定を行う場合に，帰無仮説を棄却するかどうかを判定する基準 5% あるいは 1% がよく使用される。<strong>有意水準</strong> 5% で検定を行うということは，<em>第1種の過誤</em>をおかす確率（または危険率）が 5% であることを意味する。すなわち，同様の調査・検定を行うと，20 回に 1 回は得られた結論が誤っていることを表す。「有意水準 <var>&alpha;</var> で検定すると有意な差が認められた」ということと，「危険率 <var>&alpha;</var> のもとで有意な差があるといえる」は同じような意味で使用される。
</p>

<h3><var>p</var>-Value [STEPS:Glossary]</h3>
<p>
The probability value (<strong><var>p</var>-value</strong>) of a statistical hypothesis test is the probability of getting a value of the test statistic as extreme as or more extreme than that observed by chance alone, if the null hypothesis <var>H₀</var> is true. It is the probability of wrongly rejecting the null hypothesis if it is in fact true.
</p>
<p>
It is equal to the <em>significance level</em> of the test for which we would only just reject the null hypothesis. The <var>p</var>-value is compared with the actual significance level of our test and, if it is smaller, the result is significant. That is, if the null hypothesis were to be rejected at the 5% signficance level, this would be reported as
    <math>
        <mrow>
            <mi>p</mi>
            <mo>&lt;</mo>
            <mn>0.05</mn>
        </mrow>
    </math>.
</p>
<p>
Small <var>p</var>-values suggest that the null hypothesis is unlikely to be true. The smaller it is, the more convincing is the rejection of the null hypothesis. It indicates the strength of evidence for say, rejecting the null hypothesis <var>H₀</var>, rather than simply concluding "Reject <var>H₀</var>" or "Do not reject <var>H₀</var>".
</p>

<h3>Significance Level [STEPS:Glossary]</h3>
<p>
The <strong>significance level</strong> of a statistical hypothesis test is a fixed probability of wrongly rejecting the null hypothesis <var>H₀</var>, if it is in fact true.
</p>
<p>
It is the probability of a <em>type I error</em> and is set by the investigator in relation to the consequences of such an error. That is, we want to make the significance level as small as possible in order to protect the null hypothesis and to prevent, as far as possible, the investigator from inadvertently making false claims.
Usually, the significance level is chosen to be 0.05 (or equivalently, 5%).
</p>

<h3>第1種の過誤 type I error [青木:用語辞典I]</h3>
<p>
帰無仮説が正しいにもかかわらず帰無仮説を棄却するという誤り。
</p>

<h3>第2種の過誤 type II error [青木:用語辞典I]</h3>
<p>
帰無仮説が誤っているにもかかわらず帰無仮説を採択するという誤り。
</p>

<h3>Type I Error [STEPS:Glossary]</h3>
<p>
In a hypothesis test, a type I error occurs when the null hypothesis is rejected when it is in fact true; that is, <em><var>H₀</var> is wrongly rejected</em>.
</p>
<p>
For example, in a clinical trial of a new drug, the null hypothesis might be that the new drug is no better, on average, than the current drug; i.e. <var>H₀</var>: there is no difference between the two drugs on average.
A type I error would occur if we concluded that the two drugs produced different effects when in fact there was no difference between them.
</p>
<p>
A type I error is often considered to be more serious, and therefore more important to avoid, than a <em>type II error</em>. The hypothesis test procedure is therefore adjusted so that there is a guaranteed 'low' probability of rejecting the null hypothesis wrongly; this probability is never 0.
</p>

<h3>Type II Error [STEPS:Glossary]</h3>
<p>
In a hypothesis test, a type II error occurs when the null hypothesis <var>H₀</var>, is not rejected when it is in fact false. For example, in a clinical trial of a new drug, the null hypothesis might be that the new drug is no better, on average, than the current drug; i.e. <var>H₀</var>: there is no difference between the two drugs on average.
</p>
<p>
A type II error would occur if it was concluded that the two drugs produced the same effect, i.e. there is no difference between the two drugs on average, when in fact they produced different ones.
</p>

<h3>Experimental Design [STEPS:Glossary]</h3>
<p>
We are concerned with the analysis of data generated from an experiment. It is wise to take time and effort to organise the experiment properly to ensure that the right type of data, and enough of it, is available to answer the questions of interest as clearly and efficiently as possible. This process is called <strong>experimental design</strong>.
</p>
<p>
The specific questions that the experiment is intended to answer must be clearly identified before carrying out the experiment. We should also attempt to identify known or expected sources of variability in the experimental units since one of the main aims of a designed experiment is to reduce the effect of these sources of variability on the answers to questions of interest. That is, we design the experiment in order to improve the precision of our answers.
</p>

<h3>Treatment [STEPS:Glossary]</h3>
<p>
In experiments, a <strong>treatment</strong> is something that researchers <em>administer</em> to experimantal units . For example,
</p>
<ul>
    <li>
        a corn field is divided into four, each part is 'treated' with a different fertiliser to see which produces the most corn;
    </li>
    <li>
        a teacher practices different teaching methods on different groups in her class to see which yields the best results;
    </li>
    <li>
        a doctor treats a patient with a skin condition with different creams to see which is most effective.
    </li>
</ul>
<p>
Treatments are administered to experimental units by 'level', where level implies amount or magnitude. For example, if the experimental units were given 5mg, 10mg, 15mg of a medication, those amounts would be three levels of the treatment. 'Level' is also used for categorical variables, such as Drugs A, B, and C, where the three are different kinds of drug, not different amounts of the same thing.
</p>

<h3>Factor [STEPS:Glossary]</h3>
<p>
A <strong>factor</strong> of an experiment is a <em>controlled</em> independent variable; a variable whose levels are set by the experimenter.
</p>
<p>
A factor is a general type or category of treatments. Different treatments constitute different levels of a factor. For example, three different groups of runners are subjected to different training methods. The runners are the experimental units, the training methods, the treatments, where the three types of training methods constitute three levels of the factor 'type of training'.
</p>

<h3>従属変数 dependent variable [青木:用語辞典I]</h3>
<p>
<em>回帰分析</em>において，ある1個の変数 <var>Y</var> の予測値
    <math>
        <mover>
            <mi>Y</mi>
            <mo>&Hat;</mo>
        </mover>
    </math>
が，<var>p</var> 個の変数 <var>X<sub>i</sub></var>
    <math>
        <mrow>
            <mo fence="true">&lpar;</mo>
            <mi>i</mi>
            <mo>&equals;</mo>
            <mn>1</mn>
            <mo separator="true">&comma;</mo>
            <mn>2</mn>
            <mo separator="true">&comma;</mo>
            <mi>&hellip;</mi>
            <mo separator="true">&comma;</mo>
            <mi>p</mi>
            <mo fence="true">&rpar;</mo>
        </mrow>
    </math>
によって
</p>
<figure>
    <math>
        <mover>
            <mi>Y</mi>
            <mo>&Hat;</mo>
        </mover>
        <mo>&equals;</mo>
        <msub>
            <mi>b</mi>
            <mn>0</mn>
        </msub>
        <mo>&plus;</mo>
        <msub>
            <mi>b</mi>
            <mn>1</mn>
        </msub>
        <mo>&InvisibleTimes;</mo>
        <msub>
            <mi>X</mi>
            <mn>1</mn>
        </msub>
        <mo>&plus;</mo>
        <msub>
            <mi>b</mi>
            <mn>2</mn>
        </msub>
        <mo>&InvisibleTimes;</mo>
        <msub>
            <mi>X</mi>
            <mn>2</mn>
        </msub>
        <mo>&plus;</mo>
        <mi>&hellip;</mi>
        <mo>&plus;</mo>
        <msub>
            <mi>b</mi>
            <mi>p</mi>
        </msub>
        <mo>&InvisibleTimes;</mo>
        <msub>
            <mi>X</mi>
            <mi>p</mi>
        </msub>
    </math>
</figure>
<p>
という重回帰式で定義される場合，<var>X<sub>i</sub></var> を<strong>独立変数</strong>，<var>Y</var> を<strong>従属変数</strong>と呼ぶ。判別分析においても，あるケースがどの群に属するかは独立変数に「従属」して決っているとも考えられるので同様に呼ぶ。
</p>

<h3>独立変数 independent variable [青木:用語辞典I]</h3>
<p>
説明変数（explanatory variable）や予測変数（predictor）とも呼ばれる。回帰分析において，ある1個の変数 <var>Y</var> の予測値
    <math>
        <mover>
            <mi>Y</mi>
            <mo>&Hat;</mo>
        </mover>
    </math>
が，<var>p</var> 個の変数 <var>X<sub>i</sub></var>
    <math>
        <mrow>
            <mo fence="true">&lpar;</mo>
            <mi>i</mi>
            <mo>&equals;</mo>
            <mn>1</mn>
            <mo separator="true">&comma;</mo>
            <mn>2</mn>
            <mo separator="true">&comma;</mo>
            <mi>&hellip;</mi>
            <mo separator="true">&comma;</mo>
            <mi>p</mi>
            <mo fence="true">&rpar;</mo>
        </mrow>
    </math>
によって
</p>
<figure>
    <math>
        <mover>
            <mi>Y</mi>
            <mo>&Hat;</mo>
        </mover>
        <mo>&equals;</mo>
        <msub>
            <mi>b</mi>
            <mn>0</mn>
        </msub>
        <mo>&plus;</mo>
        <msub>
            <mi>b</mi>
            <mn>1</mn>
        </msub>
        <mo>&InvisibleTimes;</mo>
        <msub>
            <mi>X</mi>
            <mn>1</mn>
        </msub>
        <mo>&plus;</mo>
        <msub>
            <mi>b</mi>
            <mn>2</mn>
        </msub>
        <mo>&InvisibleTimes;</mo>
        <msub>
            <mi>X</mi>
            <mn>2</mn>
        </msub>
        <mo>&plus;</mo>
        <mi>&hellip;</mi>
        <mo>&plus;</mo>
        <msub>
            <mi>b</mi>
            <mi>p</mi>
        </msub>
        <mo>&InvisibleTimes;</mo>
        <msub>
            <mi>X</mi>
            <mi>p</mi>
        </msub>
    </math>
</figure>
<p>
という重回帰式で定義される場合，<var>X<sub>i</sub></var> を独立変数（リグレッサー regressor），<var>Y</var> を従属変数（リグレッサンド regressand）と呼ぶ。
</p>
<p>
例えば実験などでいくつかの実験条件によって結果が変化するような場合，結果（従属変数）は実験条件（独立変数）に「従属」して決るが，実験条件は結果とは「独立」に自由に変えられるという意味を含んでいる。説明変数という呼びかたは，従属変数の変動を「説明」することから，予測変数という呼びかたは，従属変数を「予測」するための変数であることからつけられたものである。
</p>
<p>
判別分析においては，あるケースがどの群に属するかを「予測」する。例えば2群の判別の場合に， <var>n<sub>1</sub></var> と <var>n<sub>2</sub></var> を各群のケース数としたとき，一方の群に
    <math displaystyle="true">
        <mfrac>
            <msub>
                <mi>n</mi>
                <mn>2</mn>
            </msub>
            <mrow>
                <msub>
                    <mi>n</mi>
                    <mn>1</mn>
                </msub>
                <mo>&plus;</mo>
                <msub>
                    <mi>n</mi>
                    <mn>2</mn>
                </msub>
            </mrow>
        </mfrac>
    </math>，
もう一方の群に
    <math displaystyle="true">
        <mfrac>
            <mrow>
                <mo>&minus;</mo>
                <msub>
                    <mi>n</mi>
                    <mn>1</mn>
                </msub>
            </mrow>
            <mrow>
                <msub>
                    <mi>n</mi>
                    <mn>1</mn>
                </msub>
                <mo>&plus;</mo>
                <msub>
                    <mi>n</mi>
                    <mn>2</mn>
                </msub>
            </mrow>
        </mfrac>
    </math>，
という数値を与えたときの重回帰分析と，通常の線形判別分析とは等価であることが導ける。このため解析プログラムによっては，判別分析の場合にも独立変数，従属変数という呼びかたをしている。ただし，判別分析においては「独立変数」よりは「説明変数」と呼んだほうが適切かもしれない。
</p>
<p>
あるケースがどの群に属するかは，例えば臨床所見から医師が鑑別診断を下すように，統計学とは別の観点から（やや経験学的に）決められる「外的基準」である。このようなことから，「従属変数」を基準変数（criterion variable）と呼ぶ場合もある（回帰分析の場合にも独立変数が「外的基準」であることに変りはない）。
</p>

<h3>回帰分析 regression analysis [青木:用語辞典I]</h3>
<p>
ある変数を別の（複数の）変数によって予測するための<em>予測式</em>を求めるための手法。予測式は両者の関係を表すことにもなる。
</p>

<h3>Regression [STEPS:Glossary]</h3>
<ul>
    <li>
        <strong>Simple linear regression</strong> aims to find a <em>linear</em> relationship between a response variable and a possible predictor variable by the method of least squares.
    </li>
    <li>
        <strong>Multiple linear regression</strong> aims is to find a <em>linear</em> relationship between a response variable and several possible predictor variables.
    </li>
    <li>
        <strong>Nonlinear regression</strong> aims to describe the relationship between a response variable and one or more explanatory variables in a <em>non-linear</em> fashion.
    </li>
</ul>

<h3>重回帰分析 multiple linear regression analysis [青木:用語辞典I]</h3>
<p>
いくつかの変数 <var>X₁</var>，<var>X₂</var>，&hellip;，<var>X<sub>n</sub></var>（独立変数）に基づいて，別の変数 <var>Y</var>（従属変数）を予測することである。予測式として，以下のようなものを得る。つまり，独立変数の重み付け合計値で予測値
    <math>
        <mover accent="true">
            <mi>Y</mi>
            <mo>&Hat;</mo>
        </mover>
    </math>
を得る。重みは<em>偏回帰係数</em>と呼ばれる。
</p>
<figure>
    <math>
        <mover accent="true">
            <mi>Y</mi>
            <mo>&Hat;</mo>
        </mover>
        <mo>&equals;</mo>
        <mi>&beta;₀</mi>
        <mo>&plus;</mo>
        <mi>&beta;₁</mi>
        <mo>&InvisibleTimes;</mo>
        <mi>X₁</mi>
        <mo>&plus;</mo>
        <mi>&beta;₂</mi>
        <mo>&InvisibleTimes;</mo>
        <mi>X₂</mi>
        <mo>&plus;</mo>
        <mi>&hellip;</mi>
        <mo>&plus;</mo>
        <msub>
            <mi>&beta;</mi>
            <mi>p</mi>
        </msub>
        <mo>&InvisibleTimes;</mo>
        <msub>
            <mi>X</mi>
            <mi>p</mi>
        </msub>
    </math>
</figure>
<p>
独立変数が 1 個の場合は
    <math>
        <mrow>
            <mover accent="true">
                <mi>Y</mi>
                <mo>&Hat;</mo>
            </mover>
            <mo>&equals;</mo>
            <mi>&beta;₀</mi>
            <mo>&plus;</mo>
            <mi>&beta;₁</mi>
            <mo>&InvisibleTimes;</mo>
            <mi>X</mi>
        </mrow>
    </math>
のように簡単になり，特に，<em>単回帰分析</em>あるいは直線回帰と呼ばれる。
</p>

<h3>Regression Equation [STEPS:Glossary]</h3>
<p>
A regression equation allows us to express the relationship between two (or more) variables algebraically. It indicates the nature of the relationship between two (or more) variables. In particular, it indicates the <em>extent</em> to which you can predict some variables by knowing others, or the extent to which some are associated with others.
</p>
<p>
A <strong>linear regression</strong> equation is usually written
</p>
<figure>
    <math>
        <mi>Y</mi>
        <mo>&equals;</mo>
        <mi>a</mi>
        <mo>&plus;</mo>
        <mi>b</mi>
        <mo>&InvisibleTimes;</mo>
        <mi>X</mi>
        <mo>&plus;</mo>
        <mi>e</mi>
    </math>
</figure>
<p>
where
    <ul>
        <li>
            <var>Y</var> is the dependent variable
        </li>
        <li>
            <var>a</var> is the intercept
        </li>
        <li>
            <var>b</var> is the slope or <em>regression coefficient</em>
        </li>
        <li>
            <var>X</var> is the independent variable (or covariate)
        </li>
        <li>
            <var>e</var> is the <em>error term</em>
        </li>
    </ul>
</p>
<p>
The equation will specify the average magnitude of the expected change in <var>Y</var> given a change in <var>X</var>. The regression equation is often represented on a scatterplot by a regression line.
</p>

<h3>偏回帰係数 partial regression coefficient [青木:用語辞典I]</h3>
<p>
1 個の従属変数 <var>Y</var> を <var>p</var> 個の独立変数 <var>X₁</var>，<var>X₂</var>，&hellip;，<var>X<sub>p</sub></var> で予測を行うための予測式は，
<figure>
    <math>
        <mover accent="true">
            <mi>Y</mi>
            <mo>&Hat;</mo>
        </mover>
        <mo>&equals;</mo>
        <mi>&beta;₀</mi>
        <mo>&plus;</mo>
        <mi>&beta;₁</mi>
        <mo>&InvisibleTimes;</mo>
        <mi>X₁</mi>
        <mo>&plus;</mo>
        <mi>&beta;₂</mi>
        <mo>&InvisibleTimes;</mo>
        <mi>X₂</mi>
        <mo>&plus;</mo>
        <mi>&hellip;</mi>
        <mo>&plus;</mo>
        <msub>
            <mi>&beta;</mi>
            <mi>p</mi>
        </msub>
        <mo>&InvisibleTimes;</mo>
        <msub>
            <mi>X</mi>
            <mi>p</mi>
        </msub>
    </math>
</figure>
である（
    <math>
        <mover accent="true">
            <mi>Y</mi>
            <mo>&Hat;</mo>
        </mover>
    </math>
は従属変数の予測値）。ここで，
    <math>
        <mrow>
            <msub>
                <mi>b</mi>
                <mi>i</mi>
            </msub>
            <mspace width="0.5em"/>
            <mrow>
                <mo fence="true">&lpar;</mo>
                <mi>i</mi>
                <mo>&equals;</mo>
                <mn>0</mn>
                <mo separator="true">&comma;</mo>
                <mn>1</mn>
                <mo separator="true">&comma;</mo>
                <mi>&hellip;</mi>
                <mo separator="true">&comma;</mo>
                <mi>p</mi>
                <mo fence="true">&rpar;</mo>
            </mrow>
        </mrow>
    </math>
が<strong>偏回帰係数</strong>である（「偏 partial」を付けないほうがよいとの意見もある）。
</p>

<h3>標準化残差 standardised residual [青木:用語辞典I]</h3>
<p>
重回帰分析において残差（residual）は，（実測値 &minus; 予測値）で定義される。<em>残差分析</em>に用いられる場合には，標準誤差で標準化した残差が使用される。
</p>

<h3>残差分析 residual analysis [青木:用語辞典I]</h3>
<p>
重回帰分析においては，<em>誤差項</em>の分布にいくつかの仮定を前提としている。
これらの仮定が満たされているかどうかについては，予測値と実測値の乖離について検討する<strong>残差分析</strong>が行われる。残差はモデル中の誤差項とは異なった振舞いをするので，<em>標準化残差</em>が用いられる。
<dl>
    <dt>標準化残差と予測値のプロット</dt>
    <dd>
        標準化残差が一定の傾向を持っている場合，例えば実測値が大きくなるにつれ残差も大きくなる場合，あるいは途中まで増加（減少）しその後減少（増加）する場合などには，重回帰モデルの妥当性が疑わしいと判断される。
    </dd>
    <dt>標準化残差の <var>Q-Q</var> プロット</dt>
    <dd>
        理論直線から離れたプロット点が存在する場合には，重回帰モデルの妥当性が疑わしいと判断される。
    </dd>
</dl>
また，いくつか飛離れたケースがある場合には，測定ミス，データ入力ミスなどの可能性もあるので，注意深く検討すべきである。
</p>

<h3>重回帰分析の補足説明 [青木:自習ノート]</h3>
<p>
重回帰分析では，個々の説明変数は従属変数と<em>直線相関</em>関係にあることが仮定されている。個々の独立変数と従属変数の組合せで散布図を描き，直線相関から大幅にずれる独立変数は適当な変数変換をしてから用いた方がよい場合もある。
</p>
<p>
予測値と標準化残差のプロット（ 残差分析 ）により，重回帰モデルの妥当性が検証できる。
</p>
<ul>
    <li>
        予測値の大小にかかわらず標準化残差が一様に散らばっていれば重回帰モデルは妥当である。
    </li>
    <li>
        予測値が大きく（ 小さく ）なるにつれ標準化残差の大きさが変化するような場合には分散が不均一であり，重回帰モデルが妥当でないことを表している。
    </li>
    <li>
        標準化残差が曲線的な変動を見せるときは，独立変数と従属変数が曲線相関を示すので，なんらかの変数変換が必要であることを表す。
    </li>
</ul>


<h3>分散分析 analysis of variance [青木:自習ノート、回帰の分散分析]</h3>
<ul>
    <li>帰無仮説 <var>H₀</var>:
        「分析に使用した独立変数で，従属変数は説明できない」
    </li>
    <li>対立仮説 <var>H₁</var>:
        「分析に使用した独立変数で，従属変数は説明できる」
    </li>
    <li>有意水準 <var>&alpha;</var> で両側検定を行う
    </li>
</ul>
<p>
従属変数の分散は回帰によって説明できる部分と，説明できない部分に分解される。
</p>
<figure>
    <figcaption>回帰の分散分析表</figcaption>
    <table>
        <tr>
            <th>変動要因</th>
            <th>平方和</th>
            <th>自由度</th>
            <th>平均平方</th>
            <th><var>F</var>値</th>
        </tr>
        <tr>
            <td>回帰</td>
            <td> <var>S<sub>r</sub></var> </td>
            <td> <var>p</var> </td>
            <td>
                <math displaystyle="true">
                    <msub>
                        <mi>MS</mi>
                        <mi>r</mi>
                    </msub>
                    <mo>&equals;</mo>
                    <mfrac>
                        <msub>
                            <mi>S</mi>
                            <mi>r</mi>
                        </msub>
                        <mi>p</mi>
                    </mfrac>
                </math>
            </td>
            <td>
                <math displaystyle="true">
                    <mfrac>
                        <msub>
                            <mi>MS</mi>
                            <mi>r</mi>
                        </msub>
                        <msub>
                            <mi>MS</mi>
                            <mi>e</mi>
                        </msub>
                    </mfrac>
                </math>
            </td>
        </tr>
        <tr>
            <td>残差</td>
            <td> <var>S<sub>e</sub></var> </td>
            <td>
                <var>n</var> &minus; <var>p</var> &minus; 1
            </td>
            <td>
                <math displaystyle="true">
                    <msub>
                        <mi>MS</mi>
                        <mi>e</mi>
                    </msub>
                    <mo>&equals;</mo>
                    <mfrac>
                        <msub>
                            <mi>S</mi>
                            <mi>r</mi>
                        </msub>
                        <mrow>
                            <mi>n</mi>
                            <mo>&minus;</mo>
                            <mi>p</mi>
                            <mo>&minus;</mo>
                            <mn>1</mn>
                        </mrow>
                    </mfrac>
                </math>
            </td>
            <td></td>
        </tr>
        <tr>
            <td>全体</td>
            <td> <var>S<sub>t</sub></var> </td>
            <td>
                <var>n</var> &minus; 1
            </td>
            <td></td>
            <td></td>
        </tr>
    </table>
</figure>
<p>
<var>F</var> 値は自由度が
    <math>
        <mo fence="true">&lpar;</mo>
        <mi>p</mi>
        <mo separator="true">&comma;</mo>
        <mi>n</mi>
        <mo>&minus;</mo>
        <mi>p</mi>
        <mo>&minus;</mo>
        <mn>1</mn>
        <mo fence="true">&rpar;</mo>
    </math>
の <var>F</var> 分布に従う。有意確率を <var>P₀</var> とすると，
</p>
<ul>
    <li>
        <math>
            <mi>P₀</mi>
            <mo>&gt;</mo>
            <mi>&alpha;</mi>
        </math>
        のとき，帰無仮説は棄却できない。
        「分析に使用した独立変数で，従属変数は説明できるとはいえない」
    </li>
    <li>
        <math>
            <mi>P₀</mi>
            <mo>&le;</mo>
            <mi>&alpha;</mi>
        </math>
        のとき，帰無仮説を棄却する。
        「分析に使用した独立変数で，従属変数は説明できる」
    </li>
</ul>

<h3>決定係数 coefficient of determination [青木:用語辞典I]</h3>
<p>
<em>重相関係数</em>の 2 乗である。<em>寄与率</em>とも呼ばれる。独立変数が従属変数のどれくらいを説明できるかを表す。この値が低いということは，得られた重回帰式の予測能力が低いことを意味する。
</p>

<h3>重相関係数 multiple correlation coefficient [青木:用語辞典I]</h3>
<p>
実測値 <var>Y</var> と予測値
    <math>
        <mover accent="true">
            <mi>Y</mi>
            <mo>&Hat;</mo>
        </mover>
    </math>
の相関係数であり，<var>R</var> で表されることが多い。
もし，重回帰式による予測が完全ならば <var>R</var> = 1 になる。
    <math>
        <mrow>
            <mn>0</mn>
            <mo>&le;</mo>
            <mi>R</mi>
            <mo>&le;</mo>
            <mn>1</mn>
        </mrow>
    </math>
</p>

</section>

<!-- ******************************************************************** -->
<h2>Time Series</h2>

<section id="time_series" class="description">

<h3>時系列データ [統計局:DSS上級]</h3>
<p>
時系列データは、複数時点から収集したデータをもとに<em>未来</em>の傾向を探るために活用されます。このデータの特徴は、データが時点順に並べられており、この順番は変更できないことです。
</p>

<h3>時系列データの変動 [統計局:DSS上級]</h3>
<p>
時系列データは、次の４つからなると考えられています。
</p>
<ul>
    <li>
        傾向変動 (T): 長期にわたる連続的な変化、基本的な変動方向 (傾向) を表す。
    </li>
    <li>
        循環変動 (C): 数年程度の周期の一定しない上下運動、景気循環などを表す。
    </li>
    <li>
        季節変動 (S): １年周期の規則的な変動、季節性を表す。
    </li>
    <li>
        不規則変動 (I): 上記以外の変動で特に規則的でない変動を表す。
    </li>
</ul>

<h3>Time Series [STEPS:Glossary]</h3>
<p>
A <strong>time series</strong> is a sequence of observations which are ordered in time (or space). If observations are made on some phenomenon throughout time, it is most sensible to display the data in the order in which they arose, particularly since successive observations will probably be dependent. Time series are best displayed in a scatter plot. The series value <var>X</var> is plotted on the vertical axis and time t on the horizontal axis. Time is called the independent variable (in this case however, something over which you have little control). There are two kinds of time series data:
</p>
<ul>
    <li>
        Continuous, where we have an observation at every instant of time, e.g. lie detectors, electrocardiograms. We denote this using observation <var>X</var> at time <var>t</var>,
        <math>
            <mrow>
                <mi>X</mi>
                <mo>&ApplyFunction;</mo>
                <mo fence="true">&lpar;</mo>
                <mi>t</mi>
                <mo fence="true">&rpar;</mo>
            </mrow>
        </math>.
    </li>
    <li>
        Discrete, where we have an observation at (usually regularly) spaced intervals. We denote this as <var>X<sub>t</sub></var>.
    </li>
</ul>

<h3>Trend Component [STEPS:Glossary]</h3>
<p>
We want to increase our understanding of a time series by picking out its main features. One of these main features is the <strong>trend component</strong>. Descriptive techniques may be extended to forecast (predict) future values.
</p>
<p>
Trend is a long term movement in a time series. It is the underlying <em>direction</em> (an upward or downward tendency) and rate of change in a time series, when allowance has been made for the other components.
</p>
<p>
A simple way of detecting trend in seasonal data is to take averages over a certain period. If these averages change with time we can say that there is evidence of a trend in the series. There are also more formal tests to enable detection of trend in time series.
</p>
<p>
It can be helpful to model trend using straight lines, polynomials etc.
</p>

<h3>Cyclical Component [STEPS:Glossary]</h3>
<p>
We want to increase our understanding of a time series by picking out its main features. One of these main features is the <strong>cyclical component</strong>. Descriptive techniques may be extended to forecast (predict) future values.
</p>
<p>
In weekly or monthly data, the cyclical component describes any regular fluctuations.

It is a non-seasonal component which varies in a recognisable cycle.
</p>

<h3>Seasonal Component [STEPS:Glossary]</h3>
<p>
We want to increase our understanding of a time series by picking out its main features. One of these main features is the <strong>seasonal component</strong>. Descriptive techniques may be extended to forecast (predict) future values.
</p>
<p>
In weekly or monthly data, the seasonal component, often referred to as seasonality, is the component of variation in a time series which is dependent on the <em>time of year</em>. It describes any regular fluctuations with a period of less than one year. For example, the costs of various types of fruits and vegetables, unemployment figures and average daily rainfall, all show marked seasonal variation.
</p>
<p>
We are interested in comparing the seasonal effects within the years, from year to year; removing seasonal effects so that the time series is easier to cope with; and, also interested in adjusting a series for seasonal effects using various models.
<p>

<h3>Irregular Component [STEPS:Glossary]</h3>
<p>
We want to increase our understanding of a time series by picking out its main features. One of these main features is the <strong>irregular component</strong> (or '<em>noise</em>'). Descriptive techniques may be extended to forecast (predict) future values.
</p>
<p>
The irregular component is that left over when the other components of the series (trend, seasonal and cyclical) have been accounted for.
</p>

<h3>Exponential Smoothing [STEPS:Glossary]</h3>
<p>
<strong>Exponential smoothing</strong> is a smoothing technique used to reduce <em>irregularities</em> (random fluctuations) in time series data, thus providing a clearer view of the true underlying behaviour of the series. It also provides an effective means of predicting future values of the time series (forecasting).
</p>

<h3>Moving Average Smoothing [STEPS:Glossary]</h3>
<p>
A moving average is a form of average which has been adjusted to allow for seasonal or cyclical components of a time series. <strong>Moving average smoothing</strong> is a smoothing technique used to make the <em>long term trends</em> of a time series clearer.
</p>
<p>
When a variable, like the number of unemployed, or the cost of strawberries, is graphed against time, there are likely to be considerable seasonal or cyclical components in the variation. These may make it difficult to see the underlying trend. These components can be eliminated by taking a suitable moving average.
</p>
<p>
By reducing random fluctuations, moving average smoothing makes long term trends clearer.
</p>


</section>

</html>
</body>
